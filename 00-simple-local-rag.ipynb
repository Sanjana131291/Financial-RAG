{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanjana131291/Financial-RAG/blob/main/00-simple-local-rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yBh3k3MD1bB"
      },
      "source": [
        "## Requirements and setup\n",
        "\n",
        "* Local NVIDIA GPU (the original coder used a NVIDIA RTX 4090 on a Windows 11 machine) or Google Colab with access to a GPU.\n",
        "* Environment setup (see [setup details on GitHub](https://github.com/mrdbourke/simple-local-rag/?tab=readme-ov-file#setup)).\n",
        "* Data source (for example, a PDF).\n",
        "* Internet connection (to download the models, but once you have them, it'll run offline)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MSrs2O9D1bB",
        "outputId": "7be6536d-98c4-4ea7-de92-36a7cda9eb66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Running in Google Colab, installing requirements.\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting torch\n",
            "  Using cached torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.80)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.7.77)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.1)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch) (75.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Using cached torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (821.2 MB)\n",
            "Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
            "Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# Perform Google Colab installs (if running in Google Colab)\n",
        "import os\n",
        "\n",
        "if \"COLAB_GPU\" in os.environ:\n",
        "    print(\"[INFO] Running in Google Colab, installing requirements.\")\n",
        "    !pip install -U torch # requires torch 2.1.1+ (for efficient sdpa implementation)\n",
        "    !pip install PyMuPDF # for reading PDFs with Python\n",
        "    !pip install tqdm # for progress bars\n",
        "    !pip install sentence-transformers # for embedding models\n",
        "    !pip install accelerate # for quantization model loading\n",
        "    !pip install bitsandbytes # for quantizing models (less storage space)\n",
        "    !pip install flash-attn --no-build-isolation # for faster attention mechanism = faster LLM inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDmlapj2D1bC"
      },
      "source": [
        "## 1. Document/Text Processing and Embedding Creation\n",
        "\n",
        "Ingredients:\n",
        "* PDF document of choice.\n",
        "* Embedding model of choice.\n",
        "\n",
        "Steps:\n",
        "1. Import PDF document.\n",
        "2. Process text for embedding (e.g. split into chunks of sentences).\n",
        "3. Embed text chunks with embedding model.\n",
        "4. Save embeddings to file for later use (embeddings will store on file for many years or until you lose your hard drive)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_VervD9D1bC"
      },
      "source": [
        "### Import PDF Document\n",
        "\n",
        "This will work with many other kinds of documents.\n",
        "\n",
        "However, we'll start with PDF since many people have PDFs.\n",
        "\n",
        "But just keep in mind, text files, email chains, support documentation, articles and more can also work.\n",
        "\n",
        "We're going to pretend as Canadian residents and who wants answers to their queries as per guide:  [ *Self-employed Business, Professional,\n",
        "Commission, Farming, and Fishing, Income, 2024* ](https://www.canada.ca/content/dam/cra-arc/formspubs/pub/t4002/t4002-24e.pdf).\n",
        "\n",
        "There are several libraries to open PDFs with Python but I found that [PyMuPDF](https://github.com/pymupdf/pymupdf) works quite well in many cases.\n",
        "\n",
        "First we'll download the PDF if it doesn't exist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7zFNEdhD1bC",
        "outputId": "60a66867-d56a-4356-b3b4-08ab86fdf7c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File doesn't exist, downloading...\n",
            "The file has been downloaded and saved as Canada CRA Guide.pdf\n"
          ]
        }
      ],
      "source": [
        "# Download PDF file\n",
        "import os\n",
        "import requests\n",
        "\n",
        "# Get PDF document\n",
        "pdf_path = \"Canada CRA Guide.pdf\"\n",
        "\n",
        "# Download PDF if it doesn't already exist\n",
        "if not os.path.exists(pdf_path):\n",
        "  print(\"File doesn't exist, downloading...\")\n",
        "\n",
        "  # The URL of the PDF you want to download\n",
        "  url = \"https://www.canada.ca/content/dam/cra-arc/formspubs/pub/t4002/t4002-24e.pdf\"\n",
        "\n",
        "  # The local filename to save the downloaded file\n",
        "  filename = pdf_path\n",
        "\n",
        "  # Send a GET request to the URL\n",
        "  response = requests.get(url)\n",
        "\n",
        "  # Check if the request was successful\n",
        "  if response.status_code == 200:\n",
        "      # Open a file in binary write mode and save the content to it\n",
        "      with open(filename, \"wb\") as file:\n",
        "          file.write(response.content)\n",
        "      print(f\"The file has been downloaded and saved as {filename}\")\n",
        "  else:\n",
        "      print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
        "else:\n",
        "  print(f\"File {pdf_path} exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM9Ux951D1bD"
      },
      "source": [
        "PDF acquired!\n",
        "\n",
        "We can import the pages of our PDF to text by first defining the PDF path and then opening and reading it with PyMuPDF (`import fitz`).\n",
        "\n",
        "We'll write a small helper function to preprocess the text as it gets read. Note that not all text will be read in the same so keep this in mind for when you prepare your text.\n",
        "\n",
        "We'll save each page to a dictionary and then append that dictionary to a list for ease of use later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691,
          "referenced_widgets": [
            "90a7c868690049edb669f86b12e30617",
            "b583b39637114cb6a3567d5af882f58a",
            "9eea09cda3f6443d9bbc31ecc4e3987e",
            "047a93da95ca4850a7dd069d7040c548",
            "b26186172935433495261e8d8ace0c2c",
            "0742ef10f2504b19983c164c20c5f3da",
            "65a83ebe560c47118cb08cf4d22d1fa1",
            "bcf227ef689b4810ad17b7f4d71c875c",
            "cae1da4aea164f4bb71710574d679882",
            "0fedf0fd1ce743beb579c334b78bb783",
            "aa6d46699c4f40d2bff242a5c484221b"
          ]
        },
        "id": "q9PWtskBD1bD",
        "outputId": "b12a0387-0e45-4706-fb0b-6d6d4bac7730"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.26.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90a7c868690049edb669f86b12e30617"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'page_number': -41,\n",
              "  'page_char_count': 150,\n",
              "  'page_word_count': 63,\n",
              "  'page_sentence_count_raw': 2,\n",
              "  'page_token_count': 37.5,\n",
              "  'text': 'Self-employed Business, Professional,  Commission, Farming, and Fishing  Income   2024                                                T4002(E) Rev. 24'},\n",
              " {'page_number': -40,\n",
              "  'page_char_count': 2916,\n",
              "  'page_word_count': 514,\n",
              "  'page_sentence_count_raw': 15,\n",
              "  'page_token_count': 729.0,\n",
              "  'text': 'canada.ca/taxes      Find out if this guide is for you  Use this guide if you earned income as a:  ■ sole proprietor (unincorporated, self-employed individual) who is any of the following:  – business person  – professional  – commission sales person (this is different from an employee who earns commission)  – daycare in your home  – farmer  – fisher  ■ member of a:  – partnership who is a business person  – partnership who is a professional  – farming or fishing partnership  It will help you calculate your self-employment income to report on your 2024 income tax return.  Though a trust may be considered an individual, this guide is not for trusts. Do not use this guide if you are a trust or  a corporation.  If you are a trust, use Guide T4013, T3 Trust Guide.  If your business is incorporated, use Guide T4012, T2 Corporation – Income Tax Guide.  This guide contains tax information for all types of self-employment business income. However, some tax rules are not the  same for all types of business. In this document, you will find the following icons:   The briefcase icon means the information is specific to business and professional income and Form T2125, Statement of  Business or Professional Activities.   The tractor icon means the information is specific to farming and Form T2042, Statement of Farming Activities.   The fish icon means the information is specific to fishing and Form T2121, Statement of Fishing Activities.  If your business is conducting research and development (R&D) in Canada  The Scientific Research and Experimental Development (SR&ED) Program gives tax incentives to encourage Canadian  businesses of all sizes and in all sectors who conduct R&D to help create a thriving R&D culture in Canada. Learn how you  can claim those incentives by going to canada.ca/taxes-sred.   For farmers  If you are participating in the AgriStability and AgriInvest programs, you have to use the applicable guide:  ■ If you are an AgriStability and AgriInvest participant in Quebec, use this guide for your income tax return and contact  La Financière agricole du Québec at 1-800-749-3646 about AgriStability and AgriInvest participation  ■ If you are an AgriStability and AgriInvest participant in Alberta, Ontario, Saskatchewan or Prince Edward Island, use  Guide RC4060, Farming Income and the AgriStability and AgriInvest Programs Guide  ■ If you are an AgriStability and AgriInvest participant in the rest of Canada, use Guide RC4408, Farming Income and the  AgriStability and AgriInvest Programs Harmonized Guide   For fishers  You can be a self-employed fisher and also a member of one or more fishing partnerships. For instance, you may have  fished for groundfish by yourself and also have been in a lobster-fishing partnership with your child.  Generally, we consider you to be a self-employed fisher if all of the following applies to you:  ■ you participate in making a catch'}]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "!pip install PyMuPDF\n",
        "import fitz # (pymupdf, found this is better than pypdf for our use case, note: licence is AGPL-3.0, keep that in mind if you want to use any code commercially)\n",
        "from tqdm.auto import tqdm # for progress bars, requires !pip install tqdm\n",
        "\n",
        "def text_formatter(text: str) -> str:\n",
        "    \"\"\"Performs minor formatting on text.\"\"\"\n",
        "    cleaned_text = text.replace(\"\\n\", \" \").strip() # note: this might be different for each doc (best to experiment)\n",
        "\n",
        "    # Other potential text formatting functions can go here\n",
        "    return cleaned_text\n",
        "\n",
        "# Open PDF and get lines/pages\n",
        "# Note: this only focuses on text, rather than images/figures etc\n",
        "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Opens a PDF file, reads its text content page by page, and collects statistics.\n",
        "\n",
        "    Parameters:\n",
        "        pdf_path (str): The file path to the PDF document to be opened and read.\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: A list of dictionaries, each containing the page number\n",
        "        (adjusted), character count, word count, sentence count, token count, and the extracted text\n",
        "        for each page.\n",
        "    \"\"\"\n",
        "    doc = fitz.open(pdf_path)  # open a document\n",
        "    pages_and_texts = []\n",
        "    for page_number, page in tqdm(enumerate(doc)):  # iterate the document pages\n",
        "        text = page.get_text()  # get plain text encoded as UTF-8\n",
        "        text = text_formatter(text)\n",
        "        pages_and_texts.append({\"page_number\": page_number - 41,  # adjust page numbers since our PDF starts on page 42\n",
        "                                \"page_char_count\": len(text),\n",
        "                                \"page_word_count\": len(text.split(\" \")),\n",
        "                                \"page_sentence_count_raw\": len(text.split(\". \")),\n",
        "                                \"page_token_count\": len(text) / 4,  # 1 token = ~4 chars, see: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
        "                                \"text\": text})\n",
        "    return pages_and_texts\n",
        "\n",
        "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
        "pages_and_texts[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33LTGDARD1bD"
      },
      "source": [
        "Now let's get a random sample of the pages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lDNfzH5D1bD",
        "outputId": "f23e2a5e-b196-44e4-9b7e-43304c128d9c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'page_number': 20,\n",
              "  'page_char_count': 3642,\n",
              "  'page_word_count': 708,\n",
              "  'page_sentence_count_raw': 24,\n",
              "  'page_token_count': 910.5,\n",
              "  'text': '62  canada.ca/taxes      If you had employees throughout 2024 but the number of arm’s length employees you insured was less than 50% of all the  insurable persons in your business, your maximum allowable deduction is the lesser of the following two amounts:  Amount 1  Determine this amount by using the following formula:  A ÷ 365 × (B + C), where:  A is the number of days during the period of the year you insured yourself and your household members, if applicable,  but insured less than 50% of your employees  B  equals $1,500 × the number of household members 18 years of age or older insured during that period  C equals $750 × the number of household members under the age of 18 insured during that period  Amount 2  If you had at least one qualified employee, Amount 2 is the lowest cost of equivalent coverage for each qualified employee,  calculated by using the X × Y = Z formula in the previous example. If you did not have at least one qualified employee, the  limit in Amount 1 will apply.  If you had employees for part of the year  If you had at least one qualified employee for part of the year and your insurable arm’s length employees represented at  least 50% of all the insurable persons in your business, calculate your limit for that period by using the X × Y = Z formula of  “If you had employees throughout 2024.”  For the rest of the year when you had no employees or when your insurable arm’s length employees represented less  than 50% of all the insurable persons in your business, your deduction limit for that remaining period is the lesser of  Amount 1 and Amount 2, calculated in the same way as in the previous section.  Undeducted premiums  If you deduct only part of your PHSP premium at line 9804 for farming or line 9270 for fishing, and you paid the premium  in the year, you can include the undeducted balance when you calculate your non-refundable medical expense tax credit.  For more information, see “Line 33099” in your Federal Income Tax and Benefit Information.  Line 9936 – Capital cost allowance (CCA)  If you use a property you own such as a building, a motor vehicle, furniture or equipment in your business, you might be  able to claim CCA. Enter the amount of CCA you calculated on the charts found on your form. For more information on  how to fill in these charts, see Chapter 4.   Line 9898 – Total farm expenses  Enter the total of lines 9790 and 9936. Enter the business part only.  Line 9899 or 9369 – Net income (loss) before adjustments   For business and professional income, use line 9369 on Form T2125.   For farming income, use line 9899 on Form T2042.   For fishing income, use line 9369 on Form T2121.  Enter the gross income minus the total expenses. If you have a loss, enter the amount in brackets. If you are a partner in a  partnership, this amount is the net income (or loss) of all partners.   Inventory adjustments included in 2024 for farmers  Line 9941 – Optional inventory adjustment included in the current year  If you want to include an inventory amount in income, read this section.  By making the OIA, you can include in your income an amount up to the FMV of your inventory minus the MIA. You can  only make the OIA if you use the cash method. For the meaning of inventory and FMV, see “Line 9942 – Mandatory  inventory adjustment included in the current year” on page 63.  For the OIA, unlike for the MIA, the inventory does not have to be purchased inventory. It is the entire inventory you still  have at the end of your 2024 fiscal period.  Enter the amount of your OIA on line 9941. You must deduct this amount as an expense in your next fiscal period.'},\n",
              " {'page_number': 45,\n",
              "  'page_char_count': 3910,\n",
              "  'page_word_count': 831,\n",
              "  'page_sentence_count_raw': 25,\n",
              "  'page_token_count': 977.5,\n",
              "  'text': 'canada.ca/taxes   87    Capital cost calculation – Change in use  Actual cost of the property              $    1  FMV of the property    $    2  Amount from line 1    $    3  Line 2 minus line 3 (if negative, enter “0”)    $    4  Enter all capital gains deductions claimed for the capital gains  related to the depreciable property    $      × 2  = $    5  Line 4 minus line 5 (if negative, enter “0”)    $      × 1/2  = $    6  Capital cost (line 1 plus line 6)              $    7  Enter the capital cost of the property from line 7 in column 3 of Area B or C.  Note  We consider you to acquire the land for an amount equal to its FMV when you change its use. Include this amount on  line 9923, “Total cost of all land additions in the year,” in Area F.  Grants, subsidies and rebates  You should subtract from the applicable expense any rebate, grant or assistance you received. Enter the net expense on the  appropriate line of your form.  When you receive a grant, subsidy or rebate from a government or a government agency to buy depreciable property,  subtract the amount of the grant, subsidy or rebate from the property’s capital cost. Do this before you enter the capital cost  in column 3 of Area B or C. If you receive a grant, subsidy or rebate for a property after the year you disposed of the  property, subtract the amount of the grant, subsidy or rebate from the UCC of the class in which the property was included.  If you made a repayment of a grant, subsidy or rebate received for a property that you were legally required to make, add  the amount you repaid to the property’s capital cost. Do this before you enter the capital cost in column 3 of Area B or C. If  you repaid this amount after the year you disposed of the property, add the amount to the UCC of the class in which the  property was included.  You may have paid GST or HST on some of the depreciable property you acquired for your business. If so, you may have  also received an input tax credit from us. Subtract the input tax credit from the property’s capital cost. Do this before you  enter the capital cost in column 3 of Area B or C, whichever applies. If you get an input tax credit for a passenger vehicle  you use in your business, use one of these methods:  ■ For a passenger vehicle you used 90% or more of the time for your business, subtract the amount of the credit from the  vehicle’s cost before you enter its capital cost in column 3 of Area B  ■ For a passenger vehicle you used less than 90% of the time for your business, do not make an adjustment in 2024.  Instead, subtract the amount of the credit from your beginning UCC in 2025  For information on claiming input tax credits for the GST/HST you paid to buy a passenger vehicle, see GST/HST  Memorandum 8.2, General Restrictions and Limitations.  If you cannot apply the grant, credit or rebate you received to reduce a particular expense or to reduce an asset’s capital  cost, include the total on the line “Grants, credits and rebates” in the income area on Form T2121.   Input tax credits are considered government assistance. Include the amount you claimed on line 108 of your GST/HST  return only if you cannot apply the rebate, grant or assistance you received to reduce a particular expense or an asset’s  capital cost. For more information, see “GST/HST input tax credits” on page 53.  You may get an incentive from a non-government agency to buy depreciable property. For example, you may receive a tax  credit that you can use to reduce your income tax payable.  You can include the amount in income or you can subtract the amount from the capital cost of the property. If the incentive  is more than the remaining UCC in the particular class, add the excess to income:  ■ on line 8230 of Form T2125 for business and professional  ■ on line 9570 of Form T2042 for farming  ■ on line “Grants, credits and rebates” of Form T2121 for fishing'},\n",
              " {'page_number': -24,\n",
              "  'page_char_count': 2381,\n",
              "  'page_word_count': 594,\n",
              "  'page_sentence_count_raw': 20,\n",
              "  'page_token_count': 595.25,\n",
              "  'text': '18  canada.ca/taxes      A. When you buy something, make sure the seller describes the item. However, sometimes there is no description on the  receipt, as with a cash register tape. In this case, you should write what the item is on the receipt or in your  expense records.  Q. What should I do if a supplier does not want to give me a receipt?  A. When you buy something, make sure you get a receipt. Farmers or fishers must obtain documentation to support the  transactions they enter in their books and records. Your transactions may be denied if you do not have the proper  documentation to support your purchases. For more information, see Guide RC4022, General Information for  GST/HST Registrants.  Keep a record of the properties you bought and sold. This record should show who sold you the property, the cost and the  date you bought it. This information will help you calculate your capital cost allowance (CCA) and other amounts.  Chapter 4 explains how to calculate CCA.  If you sell or trade a property, show the date you sold or traded it and the amount of the payment or credit from the sale  or trade-in.    Example  The following expense journal is an example of how to record your expenses for one month:   Example of how to record fishing expenses  Summary Sheet for a Fishing Boat – Fishing on a Share Basis  Date  Gross  stock  Boat  share  Oil  Bait  Ice  Food  Captain’s  commission  Crewman  No.1  Crewman  No.2  Crewman  No.3  Crewman  No.4  Totals  February 14  $10,000  $ 4,000  $300  $400  $200  $300  $200  $1,150  $1,150  $1,150  $1,150  $10,000  March 10  $30,000  $12,000  $300  $400  $200  $300  $600  $4,050  $4,050  $4,050  $4,050  $30,000  March 19  $20,000  $ 8,000  $300  $400  $200  $300  $400  $2,600  $2,600  $2,600  $2,600  $20,000  Totals  $60,000  $24,000  $900  $1200  $600  $900  $1200  $7,800  $7,800  $7,800  $7,800  $60,000          Date  Particulars  Cheque  No.  Bank  GST   (5%)  Purchases  Legal &   Acct.  Adv.  Permit  Repairs  Capital  items  July 1  XYZ Radio  407  367.50  17.50      350.00        July 1  Smith Hardware  408  26.95  1.28          25.67    July 2  City of Ottawa  409  157.50  7.50        150.00      July 3  Andy’s Accounting  410  262.50  12.50    250.00          July 5  Wholesale Supply  Inc.  411  1,836.60  87.46  1,749.14            July 5  Ed’s Used Cars  412  1,575.00  75.00            1,500.00'}]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "random.sample(pages_and_texts, k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irQaCP1ND1bD"
      },
      "source": [
        "### Get some stats on the text\n",
        "\n",
        "Let's perform a rough exploratory data analysis (EDA) to get an idea of the size of the texts (e.g. character counts, word counts etc) we're working with.\n",
        "\n",
        "The different sizes of texts will be a good indicator into how we should split our texts.\n",
        "\n",
        "Many embedding models have limits on the size of texts they can ingest, for example, the [`sentence-transformers`](https://www.sbert.net/docs/pretrained_models.html) model [`all-mpnet-base-v2`](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) has an input size of 384 tokens.\n",
        "\n",
        "This means that the model has been trained in ingest and turn into embeddings texts with 384 tokens (1 token ~= 4 characters ~= 0.75 words).\n",
        "\n",
        "Texts over 384 tokens which are encoded by this model will be auotmatically reduced to 384 tokens in length, potentially losing some information.\n",
        "\n",
        "We'll discuss this more in the embedding section.\n",
        "\n",
        "For now, let's turn our list of dictionaries into a DataFrame and explore it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "vjTJfW7UD1bD",
        "outputId": "7617dccd-03f6-4f2a-b0ee-55b9efa00f7e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
              "0          -41              150               63                        2   \n",
              "1          -40             2916              514                       15   \n",
              "2          -39             1415              250                        7   \n",
              "3          -38             3453              577                       25   \n",
              "4          -37             1687              257                       10   \n",
              "\n",
              "   page_token_count                                               text  \n",
              "0             37.50  Self-employed Business, Professional,  Commiss...  \n",
              "1            729.00  canada.ca/taxes      Find out if this guide is...  \n",
              "2            353.75  canada.ca/taxes  ■ you are not fishing for you...  \n",
              "3            863.25  canada.ca/taxes      What’s new for 2024  New ...  \n",
              "4            421.75  canada.ca/taxes  To learn more about the repor...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f606cba5-80ad-488b-a3b5-8120380ee7b1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>page_number</th>\n",
              "      <th>page_char_count</th>\n",
              "      <th>page_word_count</th>\n",
              "      <th>page_sentence_count_raw</th>\n",
              "      <th>page_token_count</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-41</td>\n",
              "      <td>150</td>\n",
              "      <td>63</td>\n",
              "      <td>2</td>\n",
              "      <td>37.50</td>\n",
              "      <td>Self-employed Business, Professional,  Commiss...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-40</td>\n",
              "      <td>2916</td>\n",
              "      <td>514</td>\n",
              "      <td>15</td>\n",
              "      <td>729.00</td>\n",
              "      <td>canada.ca/taxes      Find out if this guide is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-39</td>\n",
              "      <td>1415</td>\n",
              "      <td>250</td>\n",
              "      <td>7</td>\n",
              "      <td>353.75</td>\n",
              "      <td>canada.ca/taxes  ■ you are not fishing for you...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-38</td>\n",
              "      <td>3453</td>\n",
              "      <td>577</td>\n",
              "      <td>25</td>\n",
              "      <td>863.25</td>\n",
              "      <td>canada.ca/taxes      What’s new for 2024  New ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-37</td>\n",
              "      <td>1687</td>\n",
              "      <td>257</td>\n",
              "      <td>10</td>\n",
              "      <td>421.75</td>\n",
              "      <td>canada.ca/taxes  To learn more about the repor...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f606cba5-80ad-488b-a3b5-8120380ee7b1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f606cba5-80ad-488b-a3b5-8120380ee7b1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f606cba5-80ad-488b-a3b5-8120380ee7b1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8d370b47-70fc-48d9-b7e7-5e95c6627467\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8d370b47-70fc-48d9-b7e7-5e95c6627467')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8d370b47-70fc-48d9-b7e7-5e95c6627467 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 113,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 32,\n        \"min\": -41,\n        \"max\": 71,\n        \"num_unique_values\": 113,\n        \"samples\": [\n          39,\n          -37,\n          -1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 927,\n        \"min\": 150,\n        \"max\": 8835,\n        \"num_unique_values\": 113,\n        \"samples\": [\n          2962,\n          1687,\n          4089\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 155,\n        \"min\": 63,\n        \"max\": 894,\n        \"num_unique_values\": 95,\n        \"samples\": [\n          735,\n          548,\n          831\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_sentence_count_raw\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12,\n        \"min\": 1,\n        \"max\": 99,\n        \"num_unique_values\": 39,\n        \"samples\": [\n          41,\n          6,\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 231.9758462221521,\n        \"min\": 37.5,\n        \"max\": 2208.75,\n        \"num_unique_values\": 113,\n        \"samples\": [\n          740.5,\n          421.75,\n          1022.25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 113,\n        \"samples\": [\n          \"canada.ca/taxes   81  Class 14  Class 14 includes patents, franchises, concessions or licences for a limited period. Your CCA is whichever of the following  amounts is less:  \\u25a0 the total of the capital cost of each property spread out over the life of the property  \\u25a0 the undepreciated capital cost to the taxpayer as of the end of the tax year of property of that class  Class 14.1 (5%)  Starting January 1, 2017, include in Class 14.1 property that:  \\u25a0 is goodwill  \\u25a0 was eligible capital property immediately before January 1, 2017, and is owned at the beginning of that day  \\u25a0 is acquired after 2016, other than:  \\u2013 property that is tangible or corporeal property  \\u2013 property that is not acquired for the purpose of gaining or producing income from business  \\u2013 property in respect of which any amount is deductible (otherwise than as a result of being included in Class 14.1) in  computing the income from the business  \\u2013 an interest in a trust  \\u2013 an interest in a partnership  \\u2013 a share, bond, debenture, mortgage, hypothecary claim, note, bill or other similar property  \\u2013 property that is an interest in, or for civil law a right in, or a right to acquire, a property described in any of the  above sub-bullets   Examples for farming are milk and egg quotas.     Examples for business, professions and fishing are franchises, concessions or licences for an unlimited period.  Properties that are included in Class 14.1 and acquired after 2016 will be included in this class at a 100% inclusion rate with  a 5% CCA rate on a declining-balance basis and the existing CCA rules will normally apply.  For tax years that end prior to 2027, properties included in Class 14.1 that were acquired before January 1, 2017, will be  depreciable at a CCA rate of 7% instead of 5%. Transitional rules will apply.  For more information about Class 14.1 and the transitional rules, see \\u201cExplanatory Notes \\u2013 Eligible Capital Property\\u201d  at budget.canada.ca/2016/docs/tm-mf/notes-en.html.  Note  Property in Class 14.1 is excluded from the definition of capital property for GST/HST purposes.  Class 16 (40%)  Class 16 includes:  \\u25a0 taxis acquired after May 25, 1976  \\u25a0 vehicles acquired after November 12, 1981, which you use in a daily car rental business  \\u25a0 coin operated video games or pinball machines acquired after February 15, 1984  \\u25a0 freight trucks acquired after December 6, 1991, that are rated above 11,788 kg  Eligible zero-emission vehicles (see the definition on page 10) are included in Class 55.  Class 43 (30%)  Include in Class 43 with a CCA rate of 30% eligible machinery and equipment used in Canada primarily to manufacture  and process goods for sale or lease that are not included in Class 29 or 53.  You can list this property in a separate class if you file an election by submitting a letter when you file your tax return for  the year in which you acquired the property. For information on separate class elections, see \\u201cClass 8 (20%)\\u201d on page 79.\",\n          \"canada.ca/taxes  To learn more about the reporting rules for digital platform operators, go to canada.ca/en/revenue-agency/programs/about  -canada-revenue-agency-cra/compliance/reporting-rules-digital-platforms.html.  Capital gains inclusion rate  The Government of Canada initially proposed to increase the inclusion rate of capital gains over $250,000 from one-half to  two-thirds after June 24, 2024. The Department of Finance announced that it will introduce legislation in Parliament in due  course, related to the capital gains inclusion rate change with a new effective date of January 1, 2026. As a result, the Canada  Revenue Agency (CRA) has reverted to administering the currently enacted capital gains inclusion rate of one-half. This  means that all capital gains realized before January 1, 2026 will be subject to the currently enacted inclusion rate of one-half,  unless an exemption applies. For more information on the inclusion rate and the change in use election, see Guide T4037,  Capital Gains. For more information on how to calculate your capital costs or deemed proceeds of dispositions, see \\u201cSpecial  situations\\u201d on page 85.  Reportable and notifiable transactions penalty  The general penalty provision for not filing an information return is removed for reportable or notifiable transactions, as  there are specific penalty provisions under the mandatory disclosure rules (MDR) that apply. This is deemed to have come  into force on June 22, 2023, which is the coming into force date of the specific penalty provisions under the MDR. For more  information, see \\u201cInformation reporting related to reportable transactions and notifiable transactions\\u201d on page 102.\",\n          \"canada.ca/taxes   41   Non-compliant short-term rentals  If you incurred expenses or used a depreciable property to earn business income from a short-term rental, you cannot  deduct from your income the non-compliant amount (see the definition on page 9) of your expenses, including any CCA  that is related to your short-term rental.  Note  To determine whether the income you earned from a short-term rental is from a property or business, consider the  number and types of services you provide for your tenants. The more services you provide, the greater the chance that  your rental operation is a business.   For more information, see Interpretation Bulletin IT-434, Rental of Real Property by Individual, and its Special Release.  If your income is from property, use Guide T4036, Rental Income, instead.  To find out how to calculate the non-compliant amount of expenses and CCA for your short-term rentals, see Guide T4036.   Line 8521 \\u2013 Advertising  You can deduct expenses for advertising, including advertising in Canadian newspapers and on Canadian television and  radio stations. You can also include any amount you paid as a finder\\u2019s fee.  To claim the expenses, you must meet certain Canadian content or Canadian ownership requirements. These requirements  do not apply if you advertise on foreign websites.  Restrictions apply to the amount of the expense you can deduct for advertising in a periodical. You can deduct all the  expense if your advertising is directed at a Canadian market and the original editorial content in the issue is 80% or more of  the issue\\u2019s total non-advertising content.  You can deduct 50% of the expense if your advertising in a periodical is directed at a Canadian market and the original  editorial content in the issue is less than 80% of the issue\\u2019s total non-advertising content.  You cannot deduct expenses for advertising directed mainly at a Canadian market when you advertise with a  foreign broadcaster.     Line 8523 \\u2013 Meals and entertainment  The maximum amount you can claim for food, beverages and entertainment expenses is 50% of the lesser of the  following amounts:  \\u25a0 the amount incurred for these expenses  \\u25a0 an amount that is reasonable in the circumstances  When you claim expenses on this line, you will have to calculate the allowable part you can claim for business use.  These limits also apply to the cost of your meals when you travel or go to a convention, conference, or similar event. Special  rules can affect your claim for meals in these cases. For more information, see \\u201cConvention expenses for business and  professional\\u201d on page 58.  These limits do not apply in any of these cases:  \\u25a0 Your business regularly provides food, beverages or entertainment to customers for compensation (for example, a  restaurant, hotel or motel)  \\u25a0 You bill your client or customer for the meal and entertainment costs, and you show these costs on the bill  \\u25a0 You include the amount of meal and entertainment expenses in an employee\\u2019s income or would include them if the  employee did not work at a remote or special work location. In addition, the amount cannot be paid or payable for a  conference, convention, seminar or similar event and the special work location must be at least 30 kilometres from the  closest urban centre with a population of 40,000 or more; visit statcan.gc.ca  \\u25a0 You incur meal and entertainment expenses for an office party or similar event, and you invite all your employees from a  particular location. The limit is six such events per year  \\u25a0 The meal and entertainment expenses you incur are for a fund-raising event that was mainly for the benefit of a  registered charity  \\u25a0 You provide meals to an employee housed at a temporary work camp constructed or installed specifically to provide  meals and accommodation to employees working at a construction site (note that the employee cannot be expected to  return home daily)  Entertainment expenses include tickets and entrance fees to an entertainment or sporting event, gratuities, cover charges  and room rentals such as hospitality suites.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(pages_and_texts)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "SWvbnOIcD1bE",
        "outputId": "a569d5ec-ed4b-47d9-ee15-a05d5666b37e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
              "count      113.000          113.000          113.000                  113.000   \n",
              "mean        15.000         3542.779          660.354                   22.805   \n",
              "std         32.764          927.903          155.278                   12.117   \n",
              "min        -41.000          150.000           63.000                    1.000   \n",
              "25%        -13.000         3210.000          590.000                   15.000   \n",
              "50%         15.000         3662.000          696.000                   22.000   \n",
              "75%         43.000         3967.000          753.000                   29.000   \n",
              "max         71.000         8835.000          894.000                   99.000   \n",
              "\n",
              "       page_token_count  \n",
              "count           113.000  \n",
              "mean            885.695  \n",
              "std             231.976  \n",
              "min              37.500  \n",
              "25%             802.500  \n",
              "50%             915.500  \n",
              "75%             991.750  \n",
              "max            2208.750  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c8f920ef-8ac8-4f4c-805a-493830de9f8e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>page_number</th>\n",
              "      <th>page_char_count</th>\n",
              "      <th>page_word_count</th>\n",
              "      <th>page_sentence_count_raw</th>\n",
              "      <th>page_token_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>113.000</td>\n",
              "      <td>113.000</td>\n",
              "      <td>113.000</td>\n",
              "      <td>113.000</td>\n",
              "      <td>113.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>15.000</td>\n",
              "      <td>3542.779</td>\n",
              "      <td>660.354</td>\n",
              "      <td>22.805</td>\n",
              "      <td>885.695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>32.764</td>\n",
              "      <td>927.903</td>\n",
              "      <td>155.278</td>\n",
              "      <td>12.117</td>\n",
              "      <td>231.976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-41.000</td>\n",
              "      <td>150.000</td>\n",
              "      <td>63.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>37.500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-13.000</td>\n",
              "      <td>3210.000</td>\n",
              "      <td>590.000</td>\n",
              "      <td>15.000</td>\n",
              "      <td>802.500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>15.000</td>\n",
              "      <td>3662.000</td>\n",
              "      <td>696.000</td>\n",
              "      <td>22.000</td>\n",
              "      <td>915.500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>43.000</td>\n",
              "      <td>3967.000</td>\n",
              "      <td>753.000</td>\n",
              "      <td>29.000</td>\n",
              "      <td>991.750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>71.000</td>\n",
              "      <td>8835.000</td>\n",
              "      <td>894.000</td>\n",
              "      <td>99.000</td>\n",
              "      <td>2208.750</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c8f920ef-8ac8-4f4c-805a-493830de9f8e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c8f920ef-8ac8-4f4c-805a-493830de9f8e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c8f920ef-8ac8-4f4c-805a-493830de9f8e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-eca46b10-d912-432c-8dc9-8d03c11f6213\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-eca46b10-d912-432c-8dc9-8d03c11f6213')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-eca46b10-d912-432c-8dc9-8d03c11f6213 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 47.935070272192156,\n        \"min\": -41.0,\n        \"max\": 113.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          113.0,\n          15.0,\n          43.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2840.758109167579,\n        \"min\": 113.0,\n        \"max\": 8835.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          3542.779,\n          3662.0,\n          113.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 327.40006559734053,\n        \"min\": 63.0,\n        \"max\": 894.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          660.354,\n          696.0,\n          113.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_sentence_count_raw\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 42.20614707006552,\n        \"min\": 1.0,\n        \"max\": 113.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          22.805,\n          22.0,\n          113.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 698.1989702923286,\n        \"min\": 37.5,\n        \"max\": 2208.75,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          885.695,\n          915.5,\n          113.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Get stats\n",
        "df.describe().round(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evP6H3iAD1bE"
      },
      "source": [
        "Okay, looks like our average token count per page is 287.\n",
        "\n",
        "For this particular use case, it means we could embed an average whole page with the `all-mpnet-base-v2` model (this model has an input capacity of 384)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf9qzNeKD1bE"
      },
      "source": [
        "### Further text processing (splitting pages into sentences)\n",
        "\n",
        "The ideal way of processing text before embedding it is still an active area of research.\n",
        "\n",
        "A simple method I've found helpful is to break the text into chunks of sentences.\n",
        "\n",
        "As in, chunk a page of text into groups of 5, 7, 10 or more sentences (these values are not set in stone and can be explored).\n",
        "\n",
        "But we want to follow the workflow of:\n",
        "\n",
        "`Ingest text -> split it into groups/chunks -> embed the groups/chunks -> use the embeddings`\n",
        "\n",
        "Some options for splitting text into sentences:\n",
        "\n",
        "1. Split into sentences with simple rules (e.g. split on \". \" with `text = text.split(\". \")`, like we did above).\n",
        "2. Split into sentences with a natural language processing (NLP) library such as [spaCy](https://spacy.io/) or [nltk](https://www.nltk.org/).\n",
        "\n",
        "Why split into sentences?\n",
        "\n",
        "* Easier to handle than larger pages of text (especially if pages are densely filled with text).\n",
        "* Can get specific and find out which group of sentences were used to help within a RAG pipeline.\n",
        "\n",
        "> **Resource:** See [spaCy install instructions](https://spacy.io/usage).\n",
        "\n",
        "Let's use spaCy to break our text into sentences since it's likely a bit more robust than just using `text.split(\". \")`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo1OcYprD1bE",
        "outputId": "c7ea0080-363a-4cf9-e703-934a8f4a1f0d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[This is a sentence., This another sentence.]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from spacy.lang.en import English # see https://spacy.io/usage for install instructions\n",
        "\n",
        "nlp = English()\n",
        "\n",
        "# Add a sentencizer pipeline, see https://spacy.io/api/sentencizer/\n",
        "nlp.add_pipe(\"sentencizer\")\n",
        "\n",
        "# Create a document instance as an example\n",
        "doc = nlp(\"This is a sentence. This another sentence.\")\n",
        "assert len(list(doc.sents)) == 2\n",
        "\n",
        "# Access the sentences of the document\n",
        "list(doc.sents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2POioHBwD1bE"
      },
      "source": [
        "We don't necessarily need to use spaCy, however, it's an open-source library designed to do NLP tasks like this at scale.\n",
        "\n",
        "So let's run our small sentencizing pipeline on our pages of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1850bdb0e5d84589a4da19aa6cbeefc8",
            "10e1e5cf93064bccaf33ed393aa43a4b",
            "8905d8a08c7a4f319ff0774827899613",
            "18117bd85de44aa0b1cd14e764aedf2c",
            "8297bf817f96432c87c9ae30e2f5de41",
            "abb3a4a0178e4a829d28589df4d83ddc",
            "0228176828294d36b50dceade40651f3",
            "b0869e408044433bbf923ccec6421e32",
            "cff986b6329143a5a2dd1b8ff342b09f",
            "748c8be1f753483d980216e421b1645a",
            "bf9e83f2fbc44b9db4485f61bc7cc9b9"
          ]
        },
        "id": "aOp1HYsFD1bE",
        "outputId": "5bc9650f-f6ab-4f8f-bc50-ff88b98ea188"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/113 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1850bdb0e5d84589a4da19aa6cbeefc8"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "for item in tqdm(pages_and_texts):\n",
        "    item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
        "\n",
        "    # Make sure all sentences are strings\n",
        "    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
        "\n",
        "    # Count the sentences\n",
        "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7TjGSXUD1bF",
        "outputId": "b2963f4d-9f10-4b81-80f0-b497bc284f1a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'page_number': 69,\n",
              "  'page_char_count': 2679,\n",
              "  'page_word_count': 441,\n",
              "  'page_sentence_count_raw': 10,\n",
              "  'page_token_count': 669.75,\n",
              "  'text': 'canada.ca/taxes  111  ■ calculate a future balance  ■ transfer payments and immediately view the updated balance  ■ make an online request about your account and view answers to common enquiries  ■ track the progress of certain files you have submitted to the CRA  ■ submit an audit enquiry  ■ download reports  ■ request relief of penalties and interest  ■ manage Multi-factor authentication settings  To sign in to or register for the CRA’s digital services, go to:  ■ My Business Account, at canada.ca/cra-sign-in-services, if you are a business owner  ■ Represent a Client, at canada.ca/cra-sign-in-services, if you are an authorized representative  For more information, go to canada.ca/taxes-business-online.  Receive your CRA mail online – Businesses  Register for email notifications to find out when CRA mail, like your notice of assessment, is available in My  Business Account.  For more information, go to canada.ca/cra-business-email-notifications.  Create a pre-authorized debit agreement for payments from your Canadian chequing account  A pre-authorized debit (PAD) is a secure online self-service payment option for individuals and businesses to pay their  taxes. A PAD lets you authorize withdrawals from your Canadian chequing account to pay the CRA. You can set the  payment dates and amounts of your PAD agreement using the CRA’s secure My Business Account service  at canada.ca/cra-sign-in-services. PADs are flexible and managed by you. You can use My Business Account to view your  account history and modify, cancel or skip a payment. For more information, go to canada.ca/pay-authorized-debit.  Electronic payments  Make your payment using:  ■ your Canadian bank or credit union’s online banking, mobile app, or telephone service  ■ the CRA’s My Payment service at canada.ca/cra-my-payment with your activated debit card from a participating  Canadian bank or credit union with one of the following logos: Visa® Debit or Debit MasterCard® (does not include credit  cards)  ■ pre-authorized debit (PAD) at canada.ca/cra-sign-in-services which lets you:  – set up payments to the CRA from a Canadian chequing account on preset dates starting in five or more business days  – pay an amount due, repay overpaid amounts, or make instalment payments  – view your account history and modify, cancel or skip a payment (for more information on PAD, go  to canada.ca/pay-authorized-debit)  ■ the “Proceed to pay” button on the “View and pay account balance” page and other pages within My Business Account  ■ your credit card, Interac e-transfer or PayPal through one of the third-party service providers for a fee  For more information, go to canada.ca/payments.',\n",
              "  'sentences': ['canada.ca/taxes  111  ■ calculate a future balance  ■ transfer payments and immediately view the updated balance  ■ make an online request about your account and view answers to common enquiries  ■ track the progress of certain files you have submitted to the CRA  ■ submit an audit enquiry  ■ download reports  ■ request relief of penalties and interest  ■ manage Multi-factor authentication settings  To sign in to or register for the CRA’s digital services, go to:  ■ My Business Account, at canada.ca/cra-sign-in-services, if you are a business owner  ■ Represent a Client, at canada.ca/cra-sign-in-services, if you are an authorized representative  For more information, go to canada.ca/taxes-business-online.',\n",
              "   ' Receive your CRA mail online – Businesses  Register for email notifications to find out when CRA mail, like your notice of assessment, is available in My  Business Account.',\n",
              "   ' For more information, go to canada.ca/cra-business-email-notifications.',\n",
              "   ' Create a pre-authorized debit agreement for payments from your Canadian chequing account  A pre-authorized debit (PAD) is a secure online self-service payment option for individuals and businesses to pay their  taxes.',\n",
              "   'A PAD lets you authorize withdrawals from your Canadian chequing account to pay the CRA.',\n",
              "   'You can set the  payment dates and amounts of your PAD agreement using the CRA’s secure My Business Account service  at canada.ca/cra-sign-in-services.',\n",
              "   'PADs are flexible and managed by you.',\n",
              "   'You can use My Business Account to view your  account history and modify, cancel or skip a payment.',\n",
              "   'For more information, go to canada.ca/pay-authorized-debit.',\n",
              "   ' Electronic payments  Make your payment using:  ■ your Canadian bank or credit union’s online banking, mobile app, or telephone service  ■ the CRA’s My Payment service at canada.ca/cra-my-payment with your activated debit card from a participating  Canadian bank or credit union with one of the following logos: Visa® Debit or Debit MasterCard® (does not include credit  cards)  ■ pre-authorized debit (PAD) at canada.ca/cra-sign-in-services which lets you:  – set up payments to the CRA from a Canadian chequing account on preset dates starting in five or more business days  – pay an amount due, repay overpaid amounts, or make instalment payments  – view your account history and modify, cancel or skip a payment (for more information on PAD, go  to canada.ca/pay-authorized-debit)  ■ the “Proceed to pay” button on the “View and pay account balance” page and other pages within My Business Account  ■ your credit card, Interac e-transfer or PayPal through one of the third-party service providers for a fee  For more information, go to canada.ca/payments.'],\n",
              "  'page_sentence_count_spacy': 10}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Inspect an example\n",
        "import random\n",
        "random.sample(pages_and_texts, k=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k2oNdg3D1bF"
      },
      "source": [
        "Wonderful!\n",
        "\n",
        "Now let's turn out list of dictionaries into a DataFrame and get some stats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "coWHQh8aD1bF",
        "outputId": "d915a288-4c13-49f6-f0cc-35e3dff4f0c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
              "count       113.00           113.00           113.00                   113.00   \n",
              "mean         15.00          3542.78           660.35                    22.81   \n",
              "std          32.76           927.90           155.28                    12.12   \n",
              "min         -41.00           150.00            63.00                     1.00   \n",
              "25%         -13.00          3210.00           590.00                    15.00   \n",
              "50%          15.00          3662.00           696.00                    22.00   \n",
              "75%          43.00          3967.00           753.00                    29.00   \n",
              "max          71.00          8835.00           894.00                    99.00   \n",
              "\n",
              "       page_token_count  page_sentence_count_spacy  \n",
              "count            113.00                     113.00  \n",
              "mean             885.69                      21.26  \n",
              "std              231.98                       9.74  \n",
              "min               37.50                       1.00  \n",
              "25%              802.50                      14.00  \n",
              "50%              915.50                      21.00  \n",
              "75%              991.75                      29.00  \n",
              "max             2208.75                      44.00  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-21fcd442-1a9b-4141-9f27-3b762858fd1c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>page_number</th>\n",
              "      <th>page_char_count</th>\n",
              "      <th>page_word_count</th>\n",
              "      <th>page_sentence_count_raw</th>\n",
              "      <th>page_token_count</th>\n",
              "      <th>page_sentence_count_spacy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>113.00</td>\n",
              "      <td>113.00</td>\n",
              "      <td>113.00</td>\n",
              "      <td>113.00</td>\n",
              "      <td>113.00</td>\n",
              "      <td>113.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>15.00</td>\n",
              "      <td>3542.78</td>\n",
              "      <td>660.35</td>\n",
              "      <td>22.81</td>\n",
              "      <td>885.69</td>\n",
              "      <td>21.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>32.76</td>\n",
              "      <td>927.90</td>\n",
              "      <td>155.28</td>\n",
              "      <td>12.12</td>\n",
              "      <td>231.98</td>\n",
              "      <td>9.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-41.00</td>\n",
              "      <td>150.00</td>\n",
              "      <td>63.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>37.50</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-13.00</td>\n",
              "      <td>3210.00</td>\n",
              "      <td>590.00</td>\n",
              "      <td>15.00</td>\n",
              "      <td>802.50</td>\n",
              "      <td>14.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>15.00</td>\n",
              "      <td>3662.00</td>\n",
              "      <td>696.00</td>\n",
              "      <td>22.00</td>\n",
              "      <td>915.50</td>\n",
              "      <td>21.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>43.00</td>\n",
              "      <td>3967.00</td>\n",
              "      <td>753.00</td>\n",
              "      <td>29.00</td>\n",
              "      <td>991.75</td>\n",
              "      <td>29.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>71.00</td>\n",
              "      <td>8835.00</td>\n",
              "      <td>894.00</td>\n",
              "      <td>99.00</td>\n",
              "      <td>2208.75</td>\n",
              "      <td>44.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-21fcd442-1a9b-4141-9f27-3b762858fd1c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-21fcd442-1a9b-4141-9f27-3b762858fd1c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-21fcd442-1a9b-4141-9f27-3b762858fd1c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-131a678c-4dc1-43e8-8b15-07b8786c3412\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-131a678c-4dc1-43e8-8b15-07b8786c3412')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-131a678c-4dc1-43e8-8b15-07b8786c3412 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 47.93503103159525,\n        \"min\": -41.0,\n        \"max\": 113.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          113.0,\n          15.0,\n          43.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2840.7584541959413,\n        \"min\": 113.0,\n        \"max\": 8835.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          3542.78,\n          3662.0,\n          113.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 327.3994766745752,\n        \"min\": 63.0,\n        \"max\": 894.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          660.35,\n          696.0,\n          113.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_sentence_count_raw\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 42.20559354855366,\n        \"min\": 1.0,\n        \"max\": 113.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          22.81,\n          22.0,\n          113.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 698.1984122815663,\n        \"min\": 37.5,\n        \"max\": 2208.75,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          885.69,\n          915.5,\n          113.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_sentence_count_spacy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 35.314364135372934,\n        \"min\": 1.0,\n        \"max\": 113.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          21.26,\n          21.0,\n          113.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df = pd.DataFrame(pages_and_texts)\n",
        "df.describe().round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvXX5UjkD1bF"
      },
      "source": [
        "For our set of text, it looks like our raw sentence count (e.g. splitting on `\". \"`) is quite close to what spaCy came up with.\n",
        "\n",
        "Now we've got our text split into sentences, how about we gorup those sentences?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qDRBdFbD1bF"
      },
      "source": [
        "### Chunking our sentences together\n",
        "\n",
        "Let's take a step to break down our list of sentences/text into smaller chunks.\n",
        "\n",
        "As you might've guessed, this process is referred to as **chunking**.\n",
        "\n",
        "Why do we do this?\n",
        "\n",
        "1. Easier to manage similar sized chunks of text.\n",
        "2. Don't overload the embedding models capacity for tokens (e.g. if an embedding model has a capacity of 384 tokens, there could be information loss if you try to embed a sequence of 400+ tokens).\n",
        "3. Our LLM context window (the amount of tokens an LLM can take in) may be limited and requires compute power so we want to make sure we're using it as well as possible.\n",
        "\n",
        "Something to note is that there are many different ways emerging for creating chunks of information/text.\n",
        "\n",
        "For now, we're going to keep it simple and break our pages of sentences into groups of 10 (this number is arbitrary and can be changed, I just picked it because it seemed to line up well with our embedding model capacity of 384).\n",
        "\n",
        "On average each of our pages has 10 sentences.\n",
        "\n",
        "And an average total of 287 tokens per page.\n",
        "\n",
        "So our groups of 10 sentences will also be ~287 tokens long.\n",
        "\n",
        "This gives us plenty of room for the text to embedded by our `all-mpnet-base-v2` model (it has a capacity of 384 tokens).\n",
        "\n",
        "To split our groups of sentences into chunks of 10 or less, let's create a function which accepts a list as input and recursively breaks into down into sublists of a specified size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUTQ2zaRD1bF"
      },
      "outputs": [],
      "source": [
        "# Define split size to turn groups of sentences into chunks\n",
        "num_sentence_chunk_size = 10\n",
        "\n",
        "# Create a function that recursively splits a list into desired sizes\n",
        "def split_list(input_list: list,\n",
        "               slice_size: int) -> list[list[str]]:\n",
        "    \"\"\"\n",
        "    Splits the input_list into sublists of size slice_size (or as close as possible).\n",
        "\n",
        "    For example, a list of 17 sentences would be split into two lists of [[10], [7]]\n",
        "    \"\"\"\n",
        "    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]\n",
        "\n",
        "# Loop through pages and texts and split sentences into chunks\n",
        "for item in tqdm(pages_and_texts):\n",
        "    item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
        "                                         slice_size=num_sentence_chunk_size)\n",
        "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeV8iPUhD1bF",
        "outputId": "64f5a3c2-8040-4565-8f72-6fdaba512fa1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'page_number': -25,\n",
              "  'page_char_count': 2643,\n",
              "  'page_word_count': 594,\n",
              "  'page_sentence_count_raw': 16,\n",
              "  'page_token_count': 660.75,\n",
              "  'text': 'canada.ca/taxes   17    Example  The following sales journal is an example of how to record your income for one month. The provincial sales tax (PST) rate  for your province is 8% and the goods and services tax (GST) rate is 5%:    Date  Particulars  Cash  sales  (1) *  Credit  sales  (2) *  Sales  returns  (3) *  Total  sales  (4) *  GST   (5%)  (5) **  PST   (8%)  (6) **  Payment on  account  (7)  1  July 1  Daily sales  146.00  27.00    173.00  8.65  13.84  10.00  2  July 2  Daily sales  167.00  36.25  26.00  177.25  8.86  14.18    3  July 3  Daily sales  155.02  19.95  10.01  164.96  8.25  13.20  32.40  4  July 4  Daily sales  147.00  29.95    176.95  8.85  14.16      *  GST and PST or harmonized sales tax (HST) are not included.  **  If you sell in one of the participating provinces, the HST replaces the GST and the PST.  In this example on July 1, you add up the sales invoices and cash register tapes. You find that you had cash sales of $146 and  sales on account of $27. In your sales journal, you record the cash sales in column 1 and the credit sales in column 2.  No merchandise was returned on July 1, so you leave column 3 blank.  In column 4, enter the total of your cash sales and your credit sales, minus merchandise returned for that day.  In columns 5 and 6, enter the total of GST and PST you charged on your sales.  In column 7, keep track of cash payments received for previous credit sales. Do not include these payments in the  daily sales figures.    Issuing receipts as a daycare provider  For daycare, you are expected to issue receipts to the parents of the children in your care. You should do this as soon as  possible to give them time to file their income tax returns. By law, the receipts you issue must include all the  following information:  ■ the name of the person you are preparing the receipt for  ■ the name of the child of the person you are preparing the receipt for  ■ the amount you received for your services  ■ the period you provided these services (from and to dates)  ■ your name  ■ your address  ■ your social insurance number  ■ your signature  ■ the date you signed the receipt  Expense records  Always get receipts or other vouchers when you buy something for your business. The receipts have to show the following:  ■ the date of the purchase  ■ the name and address of the seller or supplier  ■ the name and address of the buyer  ■ the full description of the goods or services  ■ the vendor’s business number if they are a GST/HST registrant when the purchase price is $100 or more (before tax)  You were asking?  Q. What should I do if there is no description on a receipt?',\n",
              "  'sentences': ['canada.ca/taxes   17    Example  The following sales journal is an example of how to record your income for one month.',\n",
              "   'The provincial sales tax (PST) rate  for your province is 8% and the goods and services tax (GST) rate is 5%:    Date  Particulars  Cash  sales  (1) *  Credit  sales  (2) *  Sales  returns  (3) *  Total  sales  (4) *  GST   (5%)  (5) **  PST   (8%)  (6) **  Payment on  account  (7)  1  July 1  Daily sales  146.00  27.00    173.00  8.65  13.84  10.00  2  July 2  Daily sales  167.00  36.25  26.00  177.25  8.86  14.18    3  July 3  Daily sales  155.02  19.95  10.01  164.96  8.25  13.20  32.40  4  July 4  Daily sales  147.00  29.95    176.95  8.85  14.16      *  GST and PST or harmonized sales tax (HST) are not included.',\n",
              "   ' **  If you sell in one of the participating provinces, the HST replaces the GST and the PST.',\n",
              "   ' In this example on July 1, you add up the sales invoices and cash register tapes.',\n",
              "   'You find that you had cash sales of $146 and  sales on account of $27.',\n",
              "   'In your sales journal, you record the cash sales in column 1 and the credit sales in column 2.',\n",
              "   ' No merchandise was returned on July 1, so you leave column 3 blank.',\n",
              "   ' In column 4, enter the total of your cash sales and your credit sales, minus merchandise returned for that day.',\n",
              "   ' In columns 5 and 6, enter the total of GST and PST you charged on your sales.',\n",
              "   ' In column 7, keep track of cash payments received for previous credit sales.',\n",
              "   'Do not include these payments in the  daily sales figures.',\n",
              "   '   Issuing receipts as a daycare provider  For daycare, you are expected to issue receipts to the parents of the children in your care.',\n",
              "   'You should do this as soon as  possible to give them time to file their income tax returns.',\n",
              "   'By law, the receipts you issue must include all the  following information:  ■ the name of the person you are preparing the receipt for  ■ the name of the child of the person you are preparing the receipt for  ■ the amount you received for your services  ■ the period you provided these services (from and to dates)  ■ your name  ■ your address  ■ your social insurance number  ■ your signature  ■ the date you signed the receipt  Expense records  Always get receipts or other vouchers when you buy something for your business.',\n",
              "   'The receipts have to show the following:  ■ the date of the purchase  ■ the name and address of the seller or supplier  ■ the name and address of the buyer  ■ the full description of the goods or services  ■ the vendor’s business number if they are a GST/HST registrant when the purchase price is $100 or more (before tax)  You were asking?',\n",
              "   ' Q. What should I do if there is no description on a receipt?'],\n",
              "  'page_sentence_count_spacy': 16}]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Sample an example from the group (note: many samples have only 1 chunk as they have <=10 sentences total)\n",
        "random.sample(pages_and_texts, k=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "rLBie-_RD1bG",
        "outputId": "113fdde6-e053-4a74-925e-e1071fc29ace"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
              "count       113.00           113.00           113.00                   113.00   \n",
              "mean         15.00          3542.78           660.35                    22.81   \n",
              "std          32.76           927.90           155.28                    12.12   \n",
              "min         -41.00           150.00            63.00                     1.00   \n",
              "25%         -13.00          3210.00           590.00                    15.00   \n",
              "50%          15.00          3662.00           696.00                    22.00   \n",
              "75%          43.00          3967.00           753.00                    29.00   \n",
              "max          71.00          8835.00           894.00                    99.00   \n",
              "\n",
              "       page_token_count  page_sentence_count_spacy  \n",
              "count            113.00                     113.00  \n",
              "mean             885.69                      21.26  \n",
              "std              231.98                       9.74  \n",
              "min               37.50                       1.00  \n",
              "25%              802.50                      14.00  \n",
              "50%              915.50                      21.00  \n",
              "75%              991.75                      29.00  \n",
              "max             2208.75                      44.00  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aaa29df9-c5f1-4723-aa98-e2e09b2274a7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>page_number</th>\n",
              "      <th>page_char_count</th>\n",
              "      <th>page_word_count</th>\n",
              "      <th>page_sentence_count_raw</th>\n",
              "      <th>page_token_count</th>\n",
              "      <th>page_sentence_count_spacy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>113.00</td>\n",
              "      <td>113.00</td>\n",
              "      <td>113.00</td>\n",
              "      <td>113.00</td>\n",
              "      <td>113.00</td>\n",
              "      <td>113.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>15.00</td>\n",
              "      <td>3542.78</td>\n",
              "      <td>660.35</td>\n",
              "      <td>22.81</td>\n",
              "      <td>885.69</td>\n",
              "      <td>21.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>32.76</td>\n",
              "      <td>927.90</td>\n",
              "      <td>155.28</td>\n",
              "      <td>12.12</td>\n",
              "      <td>231.98</td>\n",
              "      <td>9.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-41.00</td>\n",
              "      <td>150.00</td>\n",
              "      <td>63.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>37.50</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-13.00</td>\n",
              "      <td>3210.00</td>\n",
              "      <td>590.00</td>\n",
              "      <td>15.00</td>\n",
              "      <td>802.50</td>\n",
              "      <td>14.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>15.00</td>\n",
              "      <td>3662.00</td>\n",
              "      <td>696.00</td>\n",
              "      <td>22.00</td>\n",
              "      <td>915.50</td>\n",
              "      <td>21.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>43.00</td>\n",
              "      <td>3967.00</td>\n",
              "      <td>753.00</td>\n",
              "      <td>29.00</td>\n",
              "      <td>991.75</td>\n",
              "      <td>29.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>71.00</td>\n",
              "      <td>8835.00</td>\n",
              "      <td>894.00</td>\n",
              "      <td>99.00</td>\n",
              "      <td>2208.75</td>\n",
              "      <td>44.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aaa29df9-c5f1-4723-aa98-e2e09b2274a7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-aaa29df9-c5f1-4723-aa98-e2e09b2274a7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-aaa29df9-c5f1-4723-aa98-e2e09b2274a7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a904a2a6-f4f3-44bd-9e4c-cb02e4d646a3\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a904a2a6-f4f3-44bd-9e4c-cb02e4d646a3')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a904a2a6-f4f3-44bd-9e4c-cb02e4d646a3 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 47.93503103159525,\n        \"min\": -41.0,\n        \"max\": 113.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          113.0,\n          15.0,\n          43.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2840.7584541959413,\n        \"min\": 113.0,\n        \"max\": 8835.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          3542.78,\n          3662.0,\n          113.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 327.3994766745752,\n        \"min\": 63.0,\n        \"max\": 894.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          660.35,\n          696.0,\n          113.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_sentence_count_raw\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 42.20559354855366,\n        \"min\": 1.0,\n        \"max\": 113.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          22.81,\n          22.0,\n          113.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 698.1984122815663,\n        \"min\": 37.5,\n        \"max\": 2208.75,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          885.69,\n          915.5,\n          113.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_sentence_count_spacy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 35.314364135372934,\n        \"min\": 1.0,\n        \"max\": 113.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          21.26,\n          21.0,\n          113.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Create a DataFrame to get stats\n",
        "df = pd.DataFrame(pages_and_texts)\n",
        "df.describe().round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af7faTJ3D1bG"
      },
      "source": [
        "Note how the average number of chunks is around 1.5, this is expected since many of our pages only contain an average of 10 sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp-yBfm2D1bG"
      },
      "source": [
        "### Splitting each chunk into its own item\n",
        "\n",
        "We'd like to embed each chunk of sentences into its own numerical representation.\n",
        "\n",
        "So to keep things clean, let's create a new list of dictionaries each containing a single chunk of sentences with relative information such as page number as well statistics about each chunk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "9d0f1ff0ebe343058227c4ee3976e7b6",
            "b5cbab46ddd843a7b87612a8e9c6fff6",
            "05a26620d95741528e71bc2fabc7331f",
            "fb31e7c5831d4b1da8c93458fa3d689b",
            "9594fe8e2ae848078a6f4bdc0e208ad5",
            "0d76f769c57640d9957a69a5afe420bb",
            "5f5f202b60da4e80ab56c65b144997c8",
            "c8f0fa81730e4d5297bf2024922214c7",
            "ecc3073000ae4683ace41ddec3ae7d1a",
            "45f0e9eecfc442708a292aaa474f4742",
            "ddc4570c785f4d24b35cfd6f7b4667a5"
          ]
        },
        "id": "T7vG-cl1D1bG",
        "outputId": "645ed0bc-6b73-4fe5-b20e-eb12c68caf81"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/113 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d0f1ff0ebe343058227c4ee3976e7b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'sentence_chunks'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-18-4245212108.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpages_and_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpages_and_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msentence_chunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence_chunks\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mchunk_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mchunk_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"page_number\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"page_number\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'sentence_chunks'"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Split each chunk into its own item\n",
        "pages_and_chunks = []\n",
        "for item in tqdm(pages_and_texts):\n",
        "    for sentence_chunk in item[\"sentence_chunks\"]:\n",
        "        chunk_dict = {}\n",
        "        chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
        "\n",
        "        # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)\n",
        "        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
        "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" -> \". A\" for any full-stop/capital letter combo\n",
        "        chunk_dict[\"sentence_chunks\"] = joined_sentence_chunk\n",
        "\n",
        "        # Get stats about the chunk\n",
        "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
        "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
        "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters\n",
        "\n",
        "        pages_and_chunks.append(chunk_dict)\n",
        "\n",
        "# How many chunks do we have?\n",
        "len(pages_and_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LOjEhUkD1bG"
      },
      "outputs": [],
      "source": [
        "# View a random sample\n",
        "random.sample(pages_and_chunks, k=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK_Hii7WD1bG"
      },
      "source": [
        "Excellent!\n",
        "\n",
        "Now we've broken our whole textbook into chunks of 10 sentences or less as well as the page number they came from.\n",
        "\n",
        "This means we could reference a chunk of text and know its source.\n",
        "\n",
        "Let's get some stats about our chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEtmPP7sD1bG"
      },
      "outputs": [],
      "source": [
        "# Get stats about our chunks\n",
        "df = pd.DataFrame(pages_and_chunks)\n",
        "df.describe().round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOe_J6cUD1bH"
      },
      "source": [
        "Hmm looks like some of our chunks have quite a low token count.\n",
        "\n",
        "How about we check for samples with less than 30 tokens (about the length of a sentence) and see if they are worth keeping?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUGgb5NlD1bI"
      },
      "outputs": [],
      "source": [
        "# Show random chunks with under 30 tokens in length\n",
        "min_token_length = 30\n",
        "for row in df[df[\"chunk_token_count\"] <= min_token_length].sample(5).iterrows():\n",
        "    print(f'Chunk token count: {row[1][\"chunk_token_count\"]} | Text: {row[1][\"sentence_chunk\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssL-blOTD1bI"
      },
      "source": [
        "Looks like many of these are headers and footers of different pages.\n",
        "\n",
        "They don't seem to offer too much information.\n",
        "\n",
        "Let's filter our DataFrame/list of dictionaries to only include chunks with over 30 tokens in length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgIl03jbD1bI"
      },
      "outputs": [],
      "source": [
        "pages_and_chunks_over_min_token_len = df[df[\"chunk_token_count\"] > min_token_length].to_dict(orient=\"records\")\n",
        "pages_and_chunks_over_min_token_len[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzWSG5PJD1bJ"
      },
      "source": [
        "Smaller chunks filtered!\n",
        "\n",
        "Time to embed our chunks of text!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UDFJ4K2D1bK"
      },
      "source": [
        "### Embedding our text chunks\n",
        "\n",
        "While humans understand text, machines understand numbers best.\n",
        "\n",
        "An [embedding](https://vickiboykis.com/what_are_embeddings/index.html) is a broad concept.\n",
        "\n",
        "But one of my favourite and simple definitions is \"a useful numerical representation\".\n",
        "\n",
        "The most powerful thing about modern embeddings is that they are *learned* representations.\n",
        "\n",
        "Meaning rather than directly mapping words/tokens/characters to numbers directly (e.g. `{\"a\": 0, \"b\": 1, \"c\": 3...}`), the numerical representation of tokens is learned by going through large corpuses of text and figuring out how different tokens relate to each other.\n",
        "\n",
        "Ideally, embeddings of text will mean that similar meaning texts have similar numerical representation.\n",
        "\n",
        "> **Note:** Most modern NLP models deal with \"tokens\" which can be considered as multiple different sizes and combinations of words and characters rather than always whole words or single characters. For example, the string `\"hello world!\"` gets mapped to the token values `{15339: b'hello', 1917: b' world', 0: b'!'}` using [Byte pair encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding) (or BPE via OpenAI's [`tiktoken`](https://github.com/openai/tiktoken) library). Google has a tokenization library called [SentencePiece](https://github.com/google/sentencepiece).\n",
        "\n",
        "Our goal is to turn each of our chunks into a numerical representation (an embedding vector, where a vector is a sequence of numbers arranged in order).\n",
        "\n",
        "Once our text samples are in embedding vectors, us humans will no longer be able to understand them.\n",
        "\n",
        "However, we don't need to.\n",
        "\n",
        "The embedding vectors are for our computers to understand.\n",
        "\n",
        "We'll use our computers to find patterns in the embeddings and then we can use their text mappings to further our understanding.\n",
        "\n",
        "Enough talking, how about we import a text embedding model and see what an embedding looks like.\n",
        "\n",
        "To do so, we'll use the [`sentence-transformers`](https://www.sbert.net/docs/installation.html) library which contains many pre-trained embedding models.\n",
        "\n",
        "Specifically, we'll get the `all-mpnet-base-v2` model (you can see the model's intended use on the [Hugging Face model card](https://huggingface.co/sentence-transformers/all-mpnet-base-v2#intended-uses))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cTdFA05D1bK"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n",
        "                                      device=\"cpu\") # choose the device to load the model to (note: GPU will often be *much* faster than CPU)\n",
        "\n",
        "# Create a list of sentences to turn into numbers\n",
        "sentences = [\n",
        "    \"The Sentences Transformers library provides an easy and open-source way to create embeddings.\",\n",
        "    \"Sentences can be embedded one by one or as a list of strings.\",\n",
        "    \"Embeddings are one of the most powerful concepts in machine learning!\",\n",
        "    \"Learn to use embeddings well and you'll be well on your way to being an AI engineer.\"\n",
        "]\n",
        "\n",
        "# Sentences are encoded/embedded by calling model.encode()\n",
        "embeddings = embedding_model.encode(sentences)\n",
        "embeddings_dict = dict(zip(sentences, embeddings))\n",
        "\n",
        "# See the embeddings\n",
        "for sentence, embedding in embeddings_dict.items():\n",
        "    print(\"Sentence:\", sentence)\n",
        "    print(\"Embedding:\", embedding)\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5affqU-RD1bL"
      },
      "source": [
        "Woah! That's a lot of numbers.\n",
        "\n",
        "How about we do just once sentence?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwJ8CoLrD1bL"
      },
      "outputs": [],
      "source": [
        "single_sentence = \"Yo! How cool are embeddings?\"\n",
        "single_embedding = embedding_model.encode(single_sentence)\n",
        "print(f\"Sentence: {single_sentence}\")\n",
        "print(f\"Embedding:\\n{single_embedding}\")\n",
        "print(f\"Embedding size: {single_embedding.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HQewgJmD1bL"
      },
      "source": [
        "Nice! We've now got a way to numerically represent each of our chunks.\n",
        "\n",
        "Our embedding has a shape of `(768,)` meaning it's a vector of 768 numbers which represent our text in high-dimensional space, too many for a human to comprehend but machines love high-dimensional space.\n",
        "\n",
        "> **Note:** No matter the size of the text input to our `all-mpnet-base-v2` model, it will be turned into an embedding size of `(768,)`. This value is fixed. So whether a sentence is 1 token long or 1000 tokens long, it will be truncated/padded with zeros to size 384 and then turned into an embedding vector of size `(768,)`. Of course, other embedding models may have different input/output shapes.\n",
        "\n",
        "How about we add an embedding field to each of our chunk items?\n",
        "\n",
        "Let's start by trying to create embeddings on the CPU, we'll time it with the `%%time` magic to see how long it takes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yBb2GLDD1bL"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Uncomment to see how long it takes to create embeddings on CPU\n",
        "# # Make sure the model is on the CPU\n",
        "# embedding_model.to(\"cpu\")\n",
        "\n",
        "# # Embed each chunk one by one\n",
        "# for item in tqdm(pages_and_chunks_over_min_token_len):\n",
        "#     item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjXcR4iAD1bM"
      },
      "source": [
        "Ok not too bad... but this would take a *really* long time if we had a larger dataset.\n",
        "\n",
        "Now let's see how long it takes to create the embeddings with a GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQdFdLRoD1bM"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Send the model to the GPU\n",
        "embedding_model.to(\"cuda\") # requires a GPU installed, for reference on my local machine, I'm using a NVIDIA RTX 4090\n",
        "\n",
        "# Create embeddings one by one on the GPU\n",
        "for item in tqdm(pages_and_chunks_over_min_token_len):\n",
        "    item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv_qEACdD1bO"
      },
      "source": [
        "Woah! Looks like the embeddings get created much faster (~10x faster on my machine) on the GPU!\n",
        "\n",
        "You'll likely notice this trend with many of your deep learning workflows. If you have access to a GPU, especially a NVIDIA GPU, you should use one if you can.\n",
        "\n",
        "But what if I told you we could go faster again?\n",
        "\n",
        "You see many modern models can handle batched predictions.\n",
        "\n",
        "This means computing on multiple samples at once.\n",
        "\n",
        "Those are the types of operations where a GPU flourishes!\n",
        "\n",
        "We can perform batched operations by turning our target text samples into a single list and then passing that list to our embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkNeOLu2D1bO"
      },
      "outputs": [],
      "source": [
        "# Turn text chunks into a single list\n",
        "text_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks_over_min_token_len]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIYEK_dGD1bO"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Embed all texts in batches\n",
        "text_chunk_embeddings = embedding_model.encode(text_chunks,\n",
        "                                               batch_size=32, # you can use different batch sizes here for speed/performance, I found 32 works well for this use case\n",
        "                                               convert_to_tensor=True) # optional to return embeddings as tensor instead of array\n",
        "\n",
        "text_chunk_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRV4l92kD1bO"
      },
      "source": [
        "That's what I'm talking about!\n",
        "\n",
        "A ~4x improvement (on my GPU) in speed thanks to batched operations.\n",
        "\n",
        "So the tip here is to use a GPU when you can and use batched operations if you can too.\n",
        "\n",
        "Now let's save our chunks and their embeddings so we could import them later if we wanted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b7KGc_uD1bO"
      },
      "source": [
        "### Save embeddings to file\n",
        "\n",
        "Since creating embeddings can be a timely process (not so much for our case but it can be for more larger datasets), let's turn our `pages_and_chunks_over_min_token_len` list of dictionaries into a DataFrame and save it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdMmMHPHD1bO"
      },
      "outputs": [],
      "source": [
        "# Save embeddings to file\n",
        "text_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks_over_min_token_len)\n",
        "embeddings_df_save_path = \"text_chunks_and_embeddings_df.csv\"\n",
        "text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "589FP4sVD1bP"
      },
      "source": [
        "And we can make sure it imports nicely by loading it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiAfk157D1bP"
      },
      "outputs": [],
      "source": [
        "# Import saved file and view\n",
        "text_chunks_and_embedding_df_load = pd.read_csv(embeddings_df_save_path)\n",
        "text_chunks_and_embedding_df_load.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wgyuhf7gD1bP"
      },
      "source": [
        "### Chunking and embedding questions\n",
        "\n",
        "> **Which embedding model should I use?**\n",
        "\n",
        "This depends on many factors. My best advice is to experiment, experiment, experiment!\n",
        "\n",
        "If you want the model to run locally, you'll have to make sure it's feasible to run on your own hardware.\n",
        "\n",
        "A good place to see how different models perform on a wide range of embedding tasks is the [Hugging Face Massive Text Embedding Benchmark (MTEB) Leaderboard](https://huggingface.co/spaces/mteb/leaderboard).\n",
        "\n",
        "> **What other forms of text chunking/splitting are there?**\n",
        "\n",
        "There are a fair few options here too. We've kept it simple with groups of sentences.\n",
        "\n",
        "For more, [Pinecone has a great guide on different kinds of chunking](https://www.pinecone.io/learn/chunking-strategies/) including for different kinds of data such as markdown and LaTeX.\n",
        "\n",
        "Libraries such as [LangChain also have a good amount of in-built text splitting options](https://python.langchain.com/docs/modules/data_connection/document_transformers/).\n",
        "\n",
        "> **What should I think about when creating my embeddings?**\n",
        "\n",
        "Our model turns text inputs up to 384 tokens long in embedding vectors of size 768.\n",
        "\n",
        "Generally, the larger the vector size, the more information that gets encoded into the embedding (however, this is not always the case, as smaller, better models can outperform larger ones).\n",
        "\n",
        "Though with larger vector sizes comes larger storage and compute requirements.\n",
        "\n",
        "Our model is also relatively small (420MB) in size compared to larger models that are available.\n",
        "\n",
        "Larger models may result in better performance but will also require more compute.\n",
        "\n",
        "So some things to think about:\n",
        "* Size of input - If you need to embed longer sequences, choose a model with a larger input capacity.\n",
        "* Size of embedding vector - Larger is generally a better representation but requires more compute/storage.\n",
        "* Size of model - Larger models generally result in better embeddings but require more compute power/time to run.\n",
        "* Open or closed - Open models allow you to run them on your own hardware whereas closed models can be easier to setup but require an API call to get embeddings.\n",
        "\n",
        "> **Where should I store my embeddings?**\n",
        "\n",
        "If you've got a relatively small dataset, for example, under 100,000 examples (this number is rough and only based on first hand experience), `np.array` or `torch.tensor` can work just fine as your dataset.\n",
        "\n",
        "But if you've got a production system and want to work with 100,000+ embeddings, you may want to look into a [vector database]( https://en.wikipedia.org/wiki/Vector_database) (these have become very popular lately and there are many offerings).\n",
        "\n",
        "### Document Ingestion and Embedding Creation Extensions\n",
        "\n",
        "One major extension to the workflow above would to functionize it.\n",
        "\n",
        "Or turn it into a script.\n",
        "\n",
        "As in, take all the functionality we've created and package it into a single process (e.g. go from document -> embeddings file).\n",
        "\n",
        "So you could input a document on one end and have embeddings come out the other end. The hardest part of this is knowing what kind of preprocessing your text may need before it's turned into embeddings. Cleaner text generally means better results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCaS6GDyD1bP"
      },
      "source": [
        "## 2. RAG - Search and Answer\n",
        "\n",
        "We discussed RAG briefly in the beginning but let's quickly recap.\n",
        "\n",
        "RAG stands for Retrieval Augmented Generation.\n",
        "\n",
        "Which is another way of saying \"given a query, search for relevant resources and answer based on those resources\".\n",
        "\n",
        "Let's breakdown each step:\n",
        "* **Retrieval** - Get relevant resources given a query. For example, if the query is \"what are the macronutrients?\" the ideal results will contain information about protein, carbohydrates and fats (and possibly alcohol) rather than information about which tractors are the best for farming (though that is also cool information).\n",
        "* **Augmentation** - LLMs are capable of generating text given a prompt. However, this generated text is designed to *look* right. And it often has some correct information, however, they are prone to hallucination (generating a result that *looks* like legit text but is factually wrong). In augmentation, we pass relevant information into the prompt and get an LLM to use that relevant information as the basis of its generation.\n",
        "* **Generation** - This is where the LLM will generate a response that has been flavoured/augmented with the retrieved resources. In turn, this not only gives us a potentially more correct answer, it also gives us resources to investigate more (since we know which resources went into the prompt).\n",
        "\n",
        "The whole idea of RAG is to get an LLM to be more factually correct based on your own input as well as have a reference to where the generated output may have come from.\n",
        "\n",
        "This is an incredibly helpful tool.\n",
        "\n",
        "Let's say you had 1000s of customer support documents.\n",
        "\n",
        "You could use RAG to generate direct answers to questions with links to relevant documentation.\n",
        "\n",
        "Or you were an insurance company with large chains of claims emails.\n",
        "\n",
        "You could use RAG to answer questions about the emails with sources.\n",
        "\n",
        "One helpful analogy is to think of LLMs as calculators for words.\n",
        "\n",
        "With good inputs, the LLM can sort them into helpful outputs.\n",
        "\n",
        "How?\n",
        "\n",
        "It starts with better search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxR7sSY2D1bP"
      },
      "source": [
        "### Similarity search\n",
        "\n",
        "Similarity search or semantic search or vector search is the idea of searching on *vibe*.\n",
        "\n",
        "If this sounds like woo, woo. It's not.\n",
        "\n",
        "Perhaps searching via *meaning* is a better analogy.\n",
        "\n",
        "With keyword search, you are trying to match the string \"apple\" with the string \"apple\".\n",
        "\n",
        "Whereas with similarity/semantic search, you may want to search \"macronutrients functions\".\n",
        "\n",
        "And get back results that don't necessarily contain the words \"macronutrients functions\" but get back pieces of text that match that meaning.\n",
        "\n",
        "> **Example:** Using similarity search on our textbook data with the query \"macronutrients function\" returns a paragraph that starts with:\n",
        ">\n",
        ">*There are three classes of macronutrients: carbohydrates, lipids, and proteins. These can be metabolically processed into cellular energy. The energy from macronutrients comes from their chemical bonds. This chemical energy is converted into cellular energy that is then utilized to perform work, allowing our bodies to conduct their basic functions.*\n",
        ">\n",
        "> as the first result. How cool!\n",
        "\n",
        "If you've ever used Google, you know this kind of workflow.\n",
        "\n",
        "But now we'd like to perform that across our own data.\n",
        "\n",
        "Let's import our embeddings we created earlier (tk -link to embedding file) and prepare them for use by turning them into a tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmMOy0CND1bP"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Import texts and embedding df\n",
        "text_chunks_and_embedding_df = pd.read_csv(\"text_chunks_and_embeddings_df.csv\")\n",
        "\n",
        "# Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\n",
        "text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
        "\n",
        "# Convert texts and embedding df to list of dicts\n",
        "pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
        "\n",
        "# Convert embeddings to torch tensor and send to device (note: NumPy arrays are float64, torch tensors are float32 by default)\n",
        "embeddings = torch.tensor(np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n",
        "embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBVVauzCD1bQ"
      },
      "outputs": [],
      "source": [
        "text_chunks_and_embedding_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sexqo56_D1bQ"
      },
      "outputs": [],
      "source": [
        "embeddings[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2nkvAw7D1bQ"
      },
      "source": [
        "Nice!\n",
        "\n",
        "Now let's prepare another instance of our embedding model. Not because we have to but because we'd like to make it so you can start the notebook from the cell above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYui3qCSD1bQ"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import util, SentenceTransformer\n",
        "\n",
        "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n",
        "                                      device=device) # choose the device to load the model to"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSffeNVaD1bQ"
      },
      "source": [
        "Embedding model ready!\n",
        "\n",
        "Time to perform a semantic search.\n",
        "\n",
        "Let's say you were studying the macronutrients.\n",
        "\n",
        "And wanted to search your textbook for \"macronutrients functions\".\n",
        "\n",
        "Well, we can do so with the following steps:\n",
        "1. Define a query string (e.g. `\"macronutrients functions\"`) - note: this could be anything, specific or not.\n",
        "2. Turn the query string in an embedding with same model we used to embed our text chunks.\n",
        "3. Perform a [dot product](https://pytorch.org/docs/stable/generated/torch.dot.html) or [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) function between the text embeddings and the query embedding (we'll get to what these are shortly) to get similarity scores.\n",
        "4. Sort the results from step 3 in descending order (a higher score means more similarity in the eyes of the model) and use these values to inspect the texts.\n",
        "\n",
        "Easy!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4sTnrr3D1bQ"
      },
      "outputs": [],
      "source": [
        "# 1. Define the query\n",
        "# Note: This could be anything. But since we're working with a nutrition textbook, we'll stick with nutrition-based queries.\n",
        "query = \"macronutrients functions\"\n",
        "print(f\"Query: {query}\")\n",
        "\n",
        "# 2. Embed the query to the same numerical space as the text examples\n",
        "# Note: It's important to embed your query with the same model you embedded your examples with.\n",
        "query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "# 3. Get similarity scores with the dot product (we'll time this for fun)\n",
        "from time import perf_counter as timer\n",
        "\n",
        "start_time = timer()\n",
        "dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n",
        "end_time = timer()\n",
        "\n",
        "print(f\"Time take to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
        "\n",
        "# 4. Get the top-k results (we'll keep this to 5)\n",
        "top_results_dot_product = torch.topk(dot_scores, k=5)\n",
        "top_results_dot_product"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39KvS6CqD1bQ"
      },
      "source": [
        "Woah!! Now that was fast!\n",
        "\n",
        "~0.00008 seconds to perform a dot product comparison across 1680 embeddings on my machine (NVIDIA RTX 4090 GPU).\n",
        "\n",
        "GPUs are optimized for these kinds of operations.\n",
        "\n",
        "So even if you we're to increase our embeddings by 100x (1680 -> 168,000), an exhaustive dot product operation would happen in ~0.008 seconds (assuming linear scaling).\n",
        "\n",
        "Heck, let's try it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wktLlrAD1bR"
      },
      "outputs": [],
      "source": [
        "larger_embeddings = torch.randn(100*embeddings.shape[0], 768).to(device)\n",
        "print(f\"Embeddings shape: {larger_embeddings.shape}\")\n",
        "\n",
        "# Perform dot product across 168,000 embeddings\n",
        "start_time = timer()\n",
        "dot_scores = util.dot_score(a=query_embedding, b=larger_embeddings)[0]\n",
        "end_time = timer()\n",
        "\n",
        "print(f\"Time take to get scores on {len(larger_embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_BcgTgcD1bR"
      },
      "source": [
        "Wow. That's quick!\n",
        "\n",
        "That means we can get pretty far by just storing our embeddings in `torch.tensor` for now.\n",
        "\n",
        "However, for *much* larger datasets, we'd likely look at a dedicated vector database/indexing libraries such as [Faiss](https://github.com/facebookresearch/faiss).\n",
        "\n",
        "Let's check the results of our original similarity search.\n",
        "\n",
        "[`torch.topk`](https://pytorch.org/docs/stable/generated/torch.topk.html) returns a tuple of values (scores) and indicies for those scores.\n",
        "\n",
        "The indicies relate to which indicies in the `embeddings` tensor have what scores in relation to the query embedding (higher is better).\n",
        "\n",
        "We can use those indicies to map back to our text chunks.\n",
        "\n",
        "First, we'll define a small helper function to print out wrapped text (so it doesn't print a whole text chunk as a single line)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8ER69R7D1bR"
      },
      "outputs": [],
      "source": [
        "# Define helper function to print wrapped text\n",
        "import textwrap\n",
        "\n",
        "def print_wrapped(text, wrap_length=80):\n",
        "    wrapped_text = textwrap.fill(text, wrap_length)\n",
        "    print(wrapped_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yrz76jKhD1bR"
      },
      "source": [
        "Now we can loop through the `top_results_dot_product` tuple and match up the scores and indicies and then use those indicies to index on our `pages_and_chunks` variable to get the relevant text chunk.\n",
        "\n",
        "Sounds like a lot but we can do it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndcKFphHD1bR"
      },
      "outputs": [],
      "source": [
        "print(f\"Query: '{query}'\\n\")\n",
        "print(\"Results:\")\n",
        "# Loop through zipped together scores and indicies from torch.topk\n",
        "for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):\n",
        "    print(f\"Score: {score:.4f}\")\n",
        "    # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
        "    print(\"Text:\")\n",
        "    print_wrapped(pages_and_chunks[idx][\"sentence_chunk\"])\n",
        "    # Print the page number too so we can reference the textbook further (and check the results)\n",
        "    print(f\"Page number: {pages_and_chunks[idx]['page_number']}\")\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVZudbpcD1bR"
      },
      "source": [
        "The first result looks to have nailed it!\n",
        "\n",
        "We get a very relevant answer to our query `\"macronutrients functions\"` even though its quite vague.\n",
        "\n",
        "That's the power of semantic search!\n",
        "\n",
        "And even better, if we wanted to inspect the result further, we get the page number where the text appears.\n",
        "\n",
        "How about we check the page to verify?\n",
        "\n",
        "We can do so by loading the page number containing the highest result (page 5 but really page 5 + 41 since our PDF page numbers start on page 41)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n64g_juD1bR"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "\n",
        "# Open PDF and load target page\n",
        "pdf_path = \"human-nutrition-text.pdf\" # requires PDF to be downloaded\n",
        "doc = fitz.open(pdf_path)\n",
        "page = doc.load_page(5 + 41) # number of page (our doc starts page numbers on page 41)\n",
        "\n",
        "# Get the image of the page\n",
        "img = page.get_pixmap(dpi=300)\n",
        "\n",
        "# Optional: save the image\n",
        "#img.save(\"output_filename.png\")\n",
        "doc.close()\n",
        "\n",
        "# Convert the Pixmap to a numpy array\n",
        "img_array = np.frombuffer(img.samples_mv,\n",
        "                          dtype=np.uint8).reshape((img.h, img.w, img.n))\n",
        "\n",
        "# Display the image using Matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(13, 10))\n",
        "plt.imshow(img_array)\n",
        "plt.title(f\"Query: '{query}' | Most relevant page:\")\n",
        "plt.axis('off') # Turn off axis\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI83xmOoD1bS"
      },
      "source": [
        "Nice!\n",
        "\n",
        "Now we can do extra research if we'd like.\n",
        "\n",
        "We could repeat this workflow for any kind of query we'd like on our textbook.\n",
        "\n",
        "And it would also work for other datatypes too.\n",
        "\n",
        "We could use semantic search on customer support documents.\n",
        "\n",
        "Or email threads.\n",
        "\n",
        "Or company plans.\n",
        "\n",
        "Or our old journal entries.\n",
        "\n",
        "Almost anything!\n",
        "\n",
        "The workflow is the same:\n",
        "\n",
        "`ingest documents -> split into chunks -> embed chunks -> make a query -> embed the query -> compare query embedding to chunk embeddings`\n",
        "\n",
        "And we get relevant resources *along with* the source they came from!\n",
        "\n",
        "That's the **retrieval** part of Retrieval Augmented Generation (RAG).\n",
        "\n",
        "Before we get to the next two steps, let's take a small aside and discuss similarity measures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGmsHpySD1bS"
      },
      "source": [
        "### Similarity measures: dot product and cosine similarity\n",
        "\n",
        "Let's talk similarity measures between vectors.\n",
        "\n",
        "Specifically, embedding vectors which are representations of data with magnitude and direction in high dimensional space (our embedding vectors have 768 dimensions).\n",
        "\n",
        "Two of the most common you'll across are the dot product and cosine similarity.\n",
        "\n",
        "They are quite similar.\n",
        "\n",
        "The main difference is that cosine similarity has a normalization step.\n",
        "\n",
        "| Similarity measure | Description | Code |\n",
        "| ----- | ----- | ----- |\n",
        "| [Dot Product](https://en.wikipedia.org/wiki/Dot_product) | - Measure of magnitude and direction between two vectors<br>- Vectors that are aligned in direction and magnitude have a higher positive value<br>- Vectors that are opposite in direction and magnitude have a higher negative value | [`torch.dot`](https://pytorch.org/docs/stable/generated/torch.dot.html), [`np.dot`](https://numpy.org/doc/stable/reference/generated/numpy.dot.html), [`sentence_transformers.util.dot_score`](https://www.sbert.net/docs/package_reference/util.html#sentence_transformers.util.dot_score) |\n",
        "| [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) | - Vectors get normalized by magnitude/[Euclidean norm](https://en.wikipedia.org/wiki/Norm_(mathematics))/L2 norm so they have unit length and are compared more so on direction<br>- Vectors that are aligned in direction have a value close to 1<br>- Vectors that are opposite in direction have a value close to -1 | [`torch.nn.functional.cosine_similarity`](https://pytorch.org/docs/stable/generated/torch.nn.functional.cosine_similarity.html), [`1 - scipy.spatial.distance.cosine`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html) (subtract the distance from 1 for similarity measure), [`sentence_transformers.util.cos_sim`](https://www.sbert.net/docs/package_reference/util.html#sentence_transformers.util.cos_sim) |\n",
        "\n",
        "For text similarity, you generally want to use cosine similarity as you are after the semantic measurements (direction) rather than magnitude.\n",
        "\n",
        "In our case, our embedding model `all-mpnet-base-v2` outputs normalized outputs (see the [Hugging Face model card](https://huggingface.co/sentence-transformers/all-mpnet-base-v2#usage-huggingface-transformers) for more on this) so dot product and cosine similarity return the same results. However, dot product is faster due to not need to perform a normalize step.\n",
        "\n",
        "To make things bit more concrete, let's make simple dot product and cosine similarity functions and view their results on different vectors.\n",
        "\n",
        "> **Note:** Similarity measures between vectors and embeddings can be used on any kind of embeddings, not just text embeddings. For example, you could measure image embedding similarity or audio embedding similarity. Or with text and image models like [CLIP](https://github.com/mlfoundations/open_clip), you can measure the similarity between text and image embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLdyN2AhD1bS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def dot_product(vector1, vector2):\n",
        "    return torch.dot(vector1, vector2)\n",
        "\n",
        "def cosine_similarity(vector1, vector2):\n",
        "    dot_product = torch.dot(vector1, vector2)\n",
        "\n",
        "    # Get Euclidean/L2 norm of each vector (removes the magnitude, keeps direction)\n",
        "    norm_vector1 = torch.sqrt(torch.sum(vector1**2))\n",
        "    norm_vector2 = torch.sqrt(torch.sum(vector2**2))\n",
        "\n",
        "    return dot_product / (norm_vector1 * norm_vector2)\n",
        "\n",
        "# Example tensors\n",
        "vector1 = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
        "vector2 = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
        "vector3 = torch.tensor([4, 5, 6], dtype=torch.float32)\n",
        "vector4 = torch.tensor([-1, -2, -3], dtype=torch.float32)\n",
        "\n",
        "# Calculate dot product\n",
        "print(\"Dot product between vector1 and vector2:\", dot_product(vector1, vector2))\n",
        "print(\"Dot product between vector1 and vector3:\", dot_product(vector1, vector3))\n",
        "print(\"Dot product between vector1 and vector4:\", dot_product(vector1, vector4))\n",
        "\n",
        "# Calculate cosine similarity\n",
        "print(\"Cosine similarity between vector1 and vector2:\", cosine_similarity(vector1, vector2))\n",
        "print(\"Cosine similarity between vector1 and vector3:\", cosine_similarity(vector1, vector3))\n",
        "print(\"Cosine similarity between vector1 and vector4:\", cosine_similarity(vector1, vector4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB9_ug2CD1bS"
      },
      "source": [
        "Notice for both dot product and cosine similarity the comparisons of `vector1` and `vector2` are the opposite of `vector1` and `vector4`.\n",
        "\n",
        "Comparing `vector1` and `vector2` both equations return positive values (14 for dot product and 1.0 for cosine similarity).\n",
        "\n",
        "But comparing `vector1` and `vector4` the result is in the negative direction.\n",
        "\n",
        "This makes sense because `vector4` is the negative version of `vector1`.\n",
        "\n",
        "Whereas comparing `vector1` and `vector3` shows a different outcome.\n",
        "\n",
        "For the dot product, the value is positive and larger then the comparison of two exactly the same vectors (32 vs 14).\n",
        "\n",
        "However, for the cosine similarity, thanks to the normalization step, comparing `vector1` and `vector3` results in a postive value close to 1 but not exactly 1.\n",
        "\n",
        "It is because of this that when comparing text embeddings, cosine similarity is generally favoured as it measures the difference in direction of a pair of vectors rather than difference in magnitude.\n",
        "\n",
        "And it is this difference in direction that is more generally considered to capture the semantic meaning/vibe of the text.\n",
        "\n",
        "The good news is that as mentioned before, the outputs of our embedding model `all-mpnet-base-v2` are already normalized.\n",
        "\n",
        "So we can continue using the dot product (cosine similarity is dot product + normalization).\n",
        "\n",
        "With similarity measures explained, let's functionize our semantic search steps from above so we can repeat them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzcowSOpD1bS"
      },
      "source": [
        "### Functionizing our semantic search pipeline\n",
        "\n",
        "Let's put all of the steps from above for semantic search into a function or two so we can repeat the workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Yoh0Y5XD1bS"
      },
      "outputs": [],
      "source": [
        "def retrieve_relevant_resources(query: str,\n",
        "                                embeddings: torch.tensor,\n",
        "                                model: SentenceTransformer=embedding_model,\n",
        "                                n_resources_to_return: int=5,\n",
        "                                print_time: bool=True):\n",
        "    \"\"\"\n",
        "    Embeds a query with model and returns top k scores and indices from embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    # Embed the query\n",
        "    query_embedding = model.encode(query,\n",
        "                                   convert_to_tensor=True)\n",
        "\n",
        "    # Get dot product scores on embeddings\n",
        "    start_time = timer()\n",
        "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
        "    end_time = timer()\n",
        "\n",
        "    if print_time:\n",
        "        print(f\"[INFO] Time taken to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
        "\n",
        "    scores, indices = torch.topk(input=dot_scores,\n",
        "                                 k=n_resources_to_return)\n",
        "\n",
        "    return scores, indices\n",
        "\n",
        "def print_top_results_and_scores(query: str,\n",
        "                                 embeddings: torch.tensor,\n",
        "                                 pages_and_chunks: list[dict]=pages_and_chunks,\n",
        "                                 n_resources_to_return: int=5):\n",
        "    \"\"\"\n",
        "    Takes a query, retrieves most relevant resources and prints them out in descending order.\n",
        "\n",
        "    Note: Requires pages_and_chunks to be formatted in a specific way (see above for reference).\n",
        "    \"\"\"\n",
        "\n",
        "    scores, indices = retrieve_relevant_resources(query=query,\n",
        "                                                  embeddings=embeddings,\n",
        "                                                  n_resources_to_return=n_resources_to_return)\n",
        "\n",
        "    print(f\"Query: {query}\\n\")\n",
        "    print(\"Results:\")\n",
        "    # Loop through zipped together scores and indicies\n",
        "    for score, index in zip(scores, indices):\n",
        "        print(f\"Score: {score:.4f}\")\n",
        "        # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
        "        print_wrapped(pages_and_chunks[index][\"sentence_chunk\"])\n",
        "        # Print the page number too so we can reference the textbook further and check the results\n",
        "        print(f\"Page number: {pages_and_chunks[index]['page_number']}\")\n",
        "        print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSdyo9zRD1bS"
      },
      "source": [
        "Excellent! Now let's test our functions out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7JzzsNQD1bT"
      },
      "outputs": [],
      "source": [
        "query = \"symptoms of pellagra\"\n",
        "\n",
        "# Get just the scores and indices of top related results\n",
        "scores, indices = retrieve_relevant_resources(query=query,\n",
        "                                              embeddings=embeddings)\n",
        "scores, indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBscVyAHD1bT"
      },
      "outputs": [],
      "source": [
        "# Print out the texts of the top scores\n",
        "print_top_results_and_scores(query=query,\n",
        "                             embeddings=embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlzT0e_UD1bT"
      },
      "source": [
        "### Semantic search/vector search extensions\n",
        "\n",
        "We've covered an exmaple of using embedding vector search to find relevant results based on a query.\n",
        "\n",
        "However, you could also add to this pipeline with traditional keyword search.\n",
        "\n",
        "Many modern search systems use keyword and vector search in tandem.\n",
        "\n",
        "Our dataset is small and allows for an exhaustive search (comparing the query to *every* possible result) but if you start to work with large scale datasets with hundred of thousands, millions or even billions of vectors, you'll want to implement an index.\n",
        "\n",
        "You can think of an index as sorting your embeddings before you search through them.\n",
        "\n",
        "So it narrows down the search space.\n",
        "\n",
        "For example, it would be inefficient to search every word in the dictionary to find the word \"duck\", instead you'd go straight to the letter D, perhaps even straight to the back half of the letter D, find words close to \"duck\" before finding it.\n",
        "\n",
        "That's how an index can help search through many examples without comprimising too much on speed or quality (for more on this, check out [nearest neighbour search](https://en.wikipedia.org/wiki/Nearest_neighbor_search)).\n",
        "\n",
        "One of the most popular indexing libraries is [Faiss](https://github.com/facebookresearch/faiss).\n",
        "\n",
        "Faiss is open-source and was originally created by Facebook to deal with internet-scale vectors and implements many algorithms such as [HNSW](https://arxiv.org/abs/1603.09320) (Hierarchical Naviganle Small Worlds)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV8gpRNxD1bT"
      },
      "source": [
        "### Getting an LLM for local generation\n",
        "\n",
        "We're got our retrieval pipeline ready, let's now get the generation side of things happening.\n",
        "\n",
        "To perform generation, we're going to use a Large Language Model (LLM).\n",
        "\n",
        "LLMs are designed to generate an output given an input.\n",
        "\n",
        "In our case, we want our LLM to generate and output of text given a input of text.\n",
        "\n",
        "And more specifically, we want the output of text to be generated based on the context of relevant information to the query.\n",
        "\n",
        "The input to an LLM is often referred to as a prompt.\n",
        "\n",
        "We'll augment our prompt with a query as well as context from our textbook related to that query.\n",
        "\n",
        "> **Which LLM should I use?**\n",
        "\n",
        "There are many LLMs available.\n",
        "\n",
        "Two of the main questions to ask from this is:\n",
        "1. Do I want it to run locally?\n",
        "2. If yes, how much compute power can I dedicate?\n",
        "\n",
        "If you're after the absolute best performance, you'll likely want to use an API (not running locally) such as GPT-4 or Claude 3. However, this comes with the tradeoff of sending your data away and then awaiting a response.\n",
        "\n",
        "For our case, since we want to set up a local pipeline and run it on our own GPU, we'd answer \"yes\" to the first question and then the second question will depend on what hardware we have available.\n",
        "\n",
        "To find open-source LLMs, one great resource is the [Hugging Face open LLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n",
        "\n",
        "The leaderboard compares many of the latest and greatest LLMs on various benchmarks.\n",
        "\n",
        "Another great resource is [TheBloke on Hugging Face](https://huggingface.co/TheBloke), an account which provides an extensive range of quantized (models that have been made smaller) LLMs.\n",
        "\n",
        "A rule of thumb for LLMs (and deep learning models in general) is that the higher the number of parameters, the better the model performs.\n",
        "\n",
        "It may be tempting to go for the largest size model (e.g. a 70B parameter model rather than a 7B parameter model) but a larger size model may not be able to run on your available hardware.\n",
        "\n",
        "The following table gives an insight into how much GPU memory you'll need to load an LLM with different sizes and different levels of [numerical precision](https://en.wikipedia.org/wiki/Precision_(computer_science)).\n",
        "\n",
        "They are based on the fact that 1 float32 value (e.g. `0.69420`) requires 4 bytes of memory and 1GB is approximately 1,000,000,000 (one billion) bytes.\n",
        "\n",
        "| Model Size (Billion Parameters) | Float32 VRAM (GB) | Float16 VRAM (GB) | 8-bit VRAM (GB) | 4-bit VRAM (GB) |\n",
        "|-----|-----|-----|-----|-----|\n",
        "| 1B                              | ~4                | ~2                | ~1              | ~0.5            |\n",
        "| 7B (e.g., [Llama 2 7B](https://huggingface.co/meta-llama/Llama-2-7b), [Gemma 7B](https://huggingface.co/google/gemma-7b-it), [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1))             | ~28               | ~14               | ~7              | ~3.5            |\n",
        "| 10B                             | ~40               | ~20               | ~10             | ~5              |\n",
        "| 70B (e.g, Llama 2 70B)          | ~280              | ~140              | ~70             | ~35             |\n",
        "| 100B                            | ~400              | ~200              | ~100            | ~50             |\n",
        "| 175B                            | ~700              | ~350              | ~175            | ~87.5           |\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Note:** Loading a model in a lower precision (e.g. 8-bit instead of float16) generally lowers performance. Lower precision can help to reduce computing requirements, however sometimes the performance degradation in terms of model output can be substantial. Finding the right speed/performance tradeoff will often require many experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWjjseFQD1bT"
      },
      "source": [
        "### Checking local GPU memory availability\n",
        "\n",
        "Let's find out what hardware we've got available and see what kind of model(s) we'll be able to load.\n",
        "\n",
        "> **Note:** You can also check this with the `!nvidia-smi` command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enQEEtPQD1bT"
      },
      "outputs": [],
      "source": [
        "# Get GPU available memory\n",
        "import torch\n",
        "gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
        "gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n",
        "print(f\"Available GPU memory: {gpu_memory_gb} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQRsprazD1bU"
      },
      "source": [
        "Ok wonderful!\n",
        "\n",
        "I'm running this notebook with a NVIDIA RTX 4090, so I've got 24GB of VRAM available.\n",
        "\n",
        "However, this may be different on your end.\n",
        "\n",
        "Looking at the table above, it seems we can run a ~7-10B parameter model in float16 precision pretty comfortably.\n",
        "\n",
        "But we could also run a smaller one if we'd like.\n",
        "\n",
        "Let's try out the recently released (at the time of writing, March 2024) LLM from Google, [Gemma](https://huggingface.co/blog/gemma).\n",
        "\n",
        "Specifically, we'll use the `gemma-7b-it` version which stands for Gemma 7B Instruction-Tuned.\n",
        "\n",
        "Instruction tuning is the process of tuning a raw language model to follow instructions.\n",
        "\n",
        "These are the kind of models you'll find in most chat-based assistants such as ChatGPT, Gemini or Claude.\n",
        "\n",
        "The following table shows different amounts of GPU memory requirements for different verions of the Gemma LLMs with varying levels of precision.\n",
        "\n",
        "| Model             | Precision | Min-Memory (Bytes) | Min-Memory (MB) | Min-Memory (GB) | Recommended Memory (GB) | Hugging Face ID |\n",
        "|-------------------|-----------|----------------|-------------|-------------| ----- | ----- |\n",
        "| [Gemma 2B](https://huggingface.co/google/gemma-2b-it)          | 4-bit     | 2,106,749,952  | 2009.15     | 1.96        | ~5.0 | [`gemma-2b`](https://huggingface.co/google/gemma-2b) or [`gemma-2b-it`](https://huggingface.co/google/gemma-2b-it) for instruction tuned version |\n",
        "| Gemma 2B          | Float16   | 5,079,453,696  | 4844.14     | 4.73        | ~8.0 | Same as above |\n",
        "| [Gemma 7B](https://huggingface.co/google/gemma-7b-it)          | 4-bit     | 5,515,859,968  | 5260.33     | 5.14        | ~8.0 | [`gemma-7b`](https://huggingface.co/google/gemma-7b) or [`gemma-7b-it`](https://huggingface.co/google/gemma-7b-it) for instruction tuned version |\n",
        "| Gemma 7B          | Float16   | 17,142,470,656 | 16348.33    | 15.97       | ~19 | Same as above |\n",
        "\n",
        "> **Note:** `gemma-7b-it` means \"instruction tuned\", as in, a base LLM (`gemma-7b`) has been fine-tuned to follow instructions, similar to [`Mistral-7B-v0.1`](https://huggingface.co/mistralai/Mistral-7B-v0.1) and [`Mistral-7B-Instruct-v0.1`](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1).\n",
        ">\n",
        "> There are also further quantized and smaller variants of Gemma (and other LLMs) available in various formats such as GGUF. You can see many of these on [TheBloke account on Hugging Face](https://huggingface.co/TheBloke).\n",
        ">\n",
        "> The version of LLM you choose to use will be largely based on project requirements and experimentation.\n",
        "\n",
        "Based on the table above, let's write a simple if/else statement which recommends which Gemma variant we should look into using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DAP6qxQD1bU"
      },
      "outputs": [],
      "source": [
        "# Note: the following is Gemma focused, however, there are more and more LLMs of the 2B and 7B size appearing for local use.\n",
        "if gpu_memory_gb < 5.1:\n",
        "    print(f\"Your available GPU memory is {gpu_memory_gb}GB, you may not have enough memory to run a Gemma LLM locally without quantization.\")\n",
        "elif gpu_memory_gb < 8.1:\n",
        "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in 4-bit precision.\")\n",
        "    use_quantization_config = True\n",
        "    model_id = \"google/gemma-2b-it\"\n",
        "elif gpu_memory_gb < 19.0:\n",
        "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in float16 or Gemma 7B in 4-bit precision.\")\n",
        "    use_quantization_config = False\n",
        "    model_id = \"google/gemma-2b-it\"\n",
        "elif gpu_memory_gb > 19.0:\n",
        "    print(f\"GPU memory: {gpu_memory_gb} | Recommend model: Gemma 7B in 4-bit or float16 precision.\")\n",
        "    use_quantization_config = False\n",
        "    model_id = \"google/gemma-7b-it\"\n",
        "\n",
        "print(f\"use_quantization_config set to: {use_quantization_config}\")\n",
        "print(f\"model_id set to: {model_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTQHLnCUD1bU"
      },
      "source": [
        "### Loading an LLM locally\n",
        "\n",
        "Alright! Looks like `gemma-7b-it` it is (for my local machine with an RTX 4090, change the `model_id` and `use_quantization_config` values to suit your needs)!\n",
        "\n",
        "There are plenty of examples of how to load the model on the `gemma-7b-it` [Hugging Face model card](https://huggingface.co/google/gemma-7b-it).\n",
        "\n",
        "Good news is, the Hugging Face [`transformers`](https://huggingface.co/docs/transformers/) library has all the tools we need.\n",
        "\n",
        "To load our LLM, we're going to need a few things:\n",
        "1. A quantization config (optional) - This will determine whether or not we load the model in 4bit precision for lower memory usage. The we can create this with the [`transformers.BitsAndBytesConfig`](https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/quantization#transformers.BitsAndBytesConfig) class (requires installing the [`bitsandbytes` library](https://github.com/TimDettmers/bitsandbytes)).\n",
        "2. A model ID - This is the reference Hugging Face model ID which will determine which tokenizer and model gets used. For example `gemma-7b-it`.\n",
        "3. A tokenzier - This is what will turn our raw text into tokens ready for the model. We can create it using the [`transformers.AutoTokenzier.from_pretrained`](https://huggingface.co/docs/transformers/v4.38.2/en/model_doc/auto#transformers.AutoTokenizer) method and passing it our model ID.\n",
        "4. An LLM model - Again, using our model ID we can load a specific LLM model. To do so we can use the [`transformers.AutoModelForCausalLM.from_pretrained`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM.from_pretrained) method and passing it our model ID as well as other various parameters.\n",
        "\n",
        "As a bonus, we'll check if [Flash Attention 2](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2) is available using `transformers.utils.is_flash_attn_2_available()`. Flash Attention 2 speeds up the attention mechanism in Transformer architecture models (which is what many modern LLMs are based on, including Gemma). So if it's available and the model is supported (not all models support Flash Attention 2), we'll use it. If it's not available, you can install it by following the instructions on the [GitHub repo](https://github.com/Dao-AILab/flash-attention).\n",
        "\n",
        "> **Note:** Flash Attention 2 currently works on NVIDIA GPUs with a compute capability score of 8.0+ (Ampere, Ada Lovelace, Hopper architectures). We can check our GPU compute capability score with [`torch.cuda.get_device_capability(0)`](https://pytorch.org/docs/stable/generated/torch.cuda.get_device_capability.html).\n",
        "\n",
        "> **Note:** To get access to the Gemma models, you will have to [agree to the terms & conditions](https://huggingface.co/google/gemma-7b-it) on the Gemma model page on Hugging Face. You will then have to authorize your local machine via the [Hugging Face CLI/Hugging Face Hub `login()` function](https://huggingface.co/docs/huggingface_hub/en/quick-start#authentication). Once you've done this, you'll be able to download the models. If you're using Google Colab, you can add a [Hugging Face token](https://huggingface.co/docs/hub/en/security-tokens) to the \"Secrets\" tab.\n",
        ">\n",
        "> Downloading an LLM locally can take a fair bit of time depending on your internet connection. Gemma 7B is about a 16GB download and Gemma 2B is about a 6GB download.\n",
        "\n",
        "Let's do it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXwvblJtD1bU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers.utils import is_flash_attn_2_available\n",
        "\n",
        "# 1. Create quantization config for smaller model loading (optional)\n",
        "# Requires !pip install bitsandbytes accelerate, see: https://github.com/TimDettmers/bitsandbytes, https://huggingface.co/docs/accelerate/\n",
        "# For models that require 4-bit quantization (use this if you have low GPU memory available)\n",
        "from transformers import BitsAndBytesConfig\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
        "                                         bnb_4bit_compute_dtype=torch.float16)\n",
        "\n",
        "# Bonus: Setup Flash Attention 2 for faster inference, default to \"sdpa\" or \"scaled dot product attention\" if it's not available\n",
        "# Flash Attention 2 requires NVIDIA GPU compute capability of 8.0 or above, see: https://developer.nvidia.com/cuda-gpus\n",
        "# Requires !pip install flash-attn, see: https://github.com/Dao-AILab/flash-attention\n",
        "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
        "  attn_implementation = \"flash_attention_2\"\n",
        "else:\n",
        "  attn_implementation = \"sdpa\"\n",
        "print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n",
        "\n",
        "# 2. Pick a model we'd like to use (this will depend on how much GPU memory you have available)\n",
        "#model_id = \"google/gemma-7b-it\"\n",
        "model_id = model_id # (we already set this above)\n",
        "print(f\"[INFO] Using model_id: {model_id}\")\n",
        "\n",
        "# 3. Instantiate tokenizer (tokenizer turns text into numbers ready for the model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
        "\n",
        "# 4. Instantiate the model\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id,\n",
        "                                                 torch_dtype=torch.float16, # datatype to use, we want float16\n",
        "                                                 quantization_config=quantization_config if use_quantization_config else None,\n",
        "                                                 low_cpu_mem_usage=False, # use full memory\n",
        "                                                 attn_implementation=attn_implementation) # which attention version to use\n",
        "\n",
        "if not use_quantization_config: # quantization takes care of device setting automatically, so if it's not used, send model to GPU\n",
        "    llm_model.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3krNgx2D1bU"
      },
      "source": [
        "We've got an LLM!\n",
        "\n",
        "Let's check it out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9eR7JcxD1bU"
      },
      "outputs": [],
      "source": [
        "llm_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvt1YWAnD1bV"
      },
      "source": [
        "Ok, ok a bunch of layers ranging from embedding layers to attention layers (see the `GemmaFlashAttention2` layers!) to MLP and normalization layers.\n",
        "\n",
        "The good news is that we don't have to know too much about these to use the model.\n",
        "\n",
        "How about we get the number of parameters in our model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MASJSJtbD1bV"
      },
      "outputs": [],
      "source": [
        "def get_model_num_params(model: torch.nn.Module):\n",
        "    return sum([param.numel() for param in model.parameters()])\n",
        "\n",
        "get_model_num_params(llm_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3zUdRQbD1bV"
      },
      "source": [
        "Hmm, turns out that Gemma 7B is really Gemma 8.5B.\n",
        "\n",
        "It pays to do your own investigations!\n",
        "\n",
        "How about we get the models memory requirements?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwE7H39ED1bV"
      },
      "outputs": [],
      "source": [
        "def get_model_mem_size(model: torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Get how much memory a PyTorch model takes up.\n",
        "\n",
        "    See: https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822\n",
        "    \"\"\"\n",
        "    # Get model parameters and buffer sizes\n",
        "    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
        "    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
        "\n",
        "    # Calculate various model sizes\n",
        "    model_mem_bytes = mem_params + mem_buffers # in bytes\n",
        "    model_mem_mb = model_mem_bytes / (1024**2) # in megabytes\n",
        "    model_mem_gb = model_mem_bytes / (1024**3) # in gigabytes\n",
        "\n",
        "    return {\"model_mem_bytes\": model_mem_bytes,\n",
        "            \"model_mem_mb\": round(model_mem_mb, 2),\n",
        "            \"model_mem_gb\": round(model_mem_gb, 2)}\n",
        "\n",
        "get_model_mem_size(llm_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ffOcS6rD1bV"
      },
      "source": [
        "Nice, looks like this model takes up 15.97GB of space on the GPU.\n",
        "\n",
        "Plus a little more for the forward pass (due to all the calculations happening between the layers).\n",
        "\n",
        "Hence why I rounded it up to be ~19GB in the table above.\n",
        "\n",
        "Now let's get to the fun part, generating some text!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hxX0FgTD1bV"
      },
      "source": [
        "### Generating text with our LLM\n",
        "\n",
        "We can generate text with our LLM `model` instance by calling the [`generate()` method](https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/text_generation#transformers.GenerationConfig) (this method has plenty of options to pass into it alongside the text) on it and passing it a tokenized input.\n",
        "\n",
        "The tokenized input comes from passing a string of text to our `tokenizer`.\n",
        "\n",
        "It's important to note that you should use a tokenizer that has been paired with a model.\n",
        "\n",
        "Otherwise if you try to use a different tokenizer and then pass those inputs to a model, you will likely get errors/strange results.\n",
        "\n",
        "For some LLMs, there's a specific template you should pass to them for ideal outputs.\n",
        "\n",
        "For example, the `gemma-7b-it` model has been trained in a dialogue fashion (instruction tuning).\n",
        "\n",
        "In this case, our `tokenizer` has a [`apply_chat_template()` method](https://huggingface.co/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template) which can prepare our input text in the right format for the model.\n",
        "\n",
        "Let's try it out.\n",
        "\n",
        "> **Note:** The following demo has been modified from the Hugging Face model card for [Gemma 7B](https://huggingface.co/google/gemma-7b-it). Many similar demos of usage are available on the model cards of similar models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwHrXmvAD1bV"
      },
      "outputs": [],
      "source": [
        "input_text = \"What are the macronutrients, and what roles do they play in the human body?\"\n",
        "print(f\"Input text:\\n{input_text}\")\n",
        "\n",
        "# Create prompt template for instruction-tuned model\n",
        "dialogue_template = [\n",
        "    {\"role\": \"user\",\n",
        "     \"content\": input_text}\n",
        "]\n",
        "\n",
        "# Apply the chat template\n",
        "prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
        "                                       tokenize=False, # keep as raw text (not tokenized)\n",
        "                                       add_generation_prompt=True)\n",
        "print(f\"\\nPrompt (formatted):\\n{prompt}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fuygl4ksD1bV"
      },
      "source": [
        "Notice the scaffolding around our input text, this is the kind of turn-by-turn instruction tuning our model has gone through.\n",
        "\n",
        "Our next step is to tokenize this formatted text and pass it to our model's `generate()` method.\n",
        "\n",
        "We'll make sure our tokenized text is on the same device as our model (GPU) using `to(\"cuda\")`.\n",
        "\n",
        "Let's generate some text!\n",
        "\n",
        "We'll time it for fun with the `%%time` magic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rFkyz4ND1bW"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Tokenize the input text (turn it into numbers) and send it to GPU\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "print(f\"Model input (tokenized):\\n{input_ids}\\n\")\n",
        "\n",
        "# Generate outputs passed on the tokenized input\n",
        "# See generate docs: https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/text_generation#transformers.GenerationConfig\n",
        "outputs = llm_model.generate(**input_ids,\n",
        "                             max_new_tokens=256) # define the maximum number of new tokens to create\n",
        "print(f\"Model output (tokens):\\n{outputs[0]}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBbEpakVD1bW"
      },
      "source": [
        "Woohoo! We just generated some text on our local GPU!\n",
        "\n",
        "Well not just yet...\n",
        "\n",
        "Our LLM accepts tokens in and sends tokens back out.\n",
        "\n",
        "We can conver the output tokens to text using [`tokenizer.decode()`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcuWe9LiD1bW"
      },
      "outputs": [],
      "source": [
        "# Decode the output tokens to text\n",
        "outputs_decoded = tokenizer.decode(outputs[0])\n",
        "print(f\"Model output (decoded):\\n{outputs_decoded}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5y2wxP-D1bW"
      },
      "source": [
        "Woah! That looks like a pretty good answer.\n",
        "\n",
        "But notice how the output contains the prompt text as well?\n",
        "\n",
        "How about we do a little formatting to replace the prompt in the output text?\n",
        "\n",
        "> **Note:** `\"<bos>\"` and `\"<eos>\"` are special tokens to denote \"beginning of sentence\" and \"end of sentence\" respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBpXxIEGD1bW"
      },
      "outputs": [],
      "source": [
        "print(f\"Input text: {input_text}\\n\")\n",
        "print(f\"Output text:\\n{outputs_decoded.replace(prompt, '').replace('<bos>', '').replace('<eos>', '')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmhynnrCD1bW"
      },
      "source": [
        "How cool is that!\n",
        "\n",
        "We just officially generated text from an LLM running locally.\n",
        "\n",
        "So we've covered the R (retrieval) and G (generation) of RAG.\n",
        "\n",
        "How about we check out the last step?\n",
        "\n",
        "Augmentation.\n",
        "\n",
        "First, let's put together a list of queries we can try out with our pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSjGMP2OD1bW"
      },
      "outputs": [],
      "source": [
        "# Nutrition-style questions generated with GPT4\n",
        "gpt4_questions = [\n",
        "    \"What are the macronutrients, and what roles do they play in the human body?\",\n",
        "    \"How do vitamins and minerals differ in their roles and importance for health?\",\n",
        "    \"Describe the process of digestion and absorption of nutrients in the human body.\",\n",
        "    \"What role does fibre play in digestion? Name five fibre containing foods.\",\n",
        "    \"Explain the concept of energy balance and its importance in weight management.\"\n",
        "]\n",
        "\n",
        "# Manually created question list\n",
        "manual_questions = [\n",
        "    \"How often should infants be breastfed?\",\n",
        "    \"What are symptoms of pellagra?\",\n",
        "    \"How does saliva help with digestion?\",\n",
        "    \"What is the RDI for protein per day?\",\n",
        "    \"water soluble vitamins\"\n",
        "]\n",
        "\n",
        "query_list = gpt4_questions + manual_questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSXWwNJXD1bX"
      },
      "source": [
        "And now let's check if our `retrieve_relevant_resources()` function works with our list of queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTJvT3fcD1bX"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "query = random.choice(query_list)\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "\n",
        "# Get just the scores and indices of top related results\n",
        "scores, indices = retrieve_relevant_resources(query=query,\n",
        "                                              embeddings=embeddings)\n",
        "scores, indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bebVAMYsD1bX"
      },
      "source": [
        "Beautiful!\n",
        "\n",
        "Let's augment!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EewXt0OlD1bX"
      },
      "source": [
        "### Augmenting our prompt with context items\n",
        "\n",
        "What we'd like to do with augmentation is take the results from our search for relevant resources and put them into the prompt that we pass to our LLM.\n",
        "\n",
        "In essence, we start with a base prompt and update it with context text.\n",
        "\n",
        "Let's write a function called `prompt_formatter` that takes in a query and our list of context items (in our case it'll be select indices from our list of dictionaries inside `pages_and_chunks`) and then formats the query with text from the context items.\n",
        "\n",
        "We'll apply the dialogue and chat template to our prompt before returning it as well.\n",
        "\n",
        "> **Note:** The process of augmenting or changing a prompt to an LLM is known as prompt engineering. And the best way to do it is an active area of research. For a comprehensive guide on different prompt engineering techniques, I'd recommend the Prompt Engineering Guide ([promptingguide.ai](https://www.promptingguide.ai/)), [Brex's Prompt Engineering Guide](https://github.com/brexhq/prompt-engineering) and the paper [Prompt Design and Engineering: Introduction and Advanced Models](https://arxiv.org/abs/2401.14423)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DquhxAxmD1bX"
      },
      "outputs": [],
      "source": [
        "def prompt_formatter(query: str,\n",
        "                     context_items: list[dict]) -> str:\n",
        "    \"\"\"\n",
        "    Augments query with text-based context from context_items.\n",
        "    \"\"\"\n",
        "    # Join context items into one dotted paragraph\n",
        "    context = \"- \" + \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n",
        "\n",
        "    # Create a base prompt with examples to help the model\n",
        "    # Note: this is very customizable, I've chosen to use 3 examples of the answer style we'd like.\n",
        "    # We could also write this in a txt file and import it in if we wanted.\n",
        "    base_prompt = \"\"\"Based on the following context items, please answer the query.\n",
        "Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
        "Don't return the thinking, only return the answer.\n",
        "Make sure your answers are as explanatory as possible.\n",
        "Use the following examples as reference for the ideal answer style.\n",
        "\\nExample 1:\n",
        "Query: What are the fat-soluble vitamins?\n",
        "Answer: The fat-soluble vitamins include Vitamin A, Vitamin D, Vitamin E, and Vitamin K. These vitamins are absorbed along with fats in the diet and can be stored in the body's fatty tissue and liver for later use. Vitamin A is important for vision, immune function, and skin health. Vitamin D plays a critical role in calcium absorption and bone health. Vitamin E acts as an antioxidant, protecting cells from damage. Vitamin K is essential for blood clotting and bone metabolism.\n",
        "\\nExample 2:\n",
        "Query: What are the causes of type 2 diabetes?\n",
        "Answer: Type 2 diabetes is often associated with overnutrition, particularly the overconsumption of calories leading to obesity. Factors include a diet high in refined sugars and saturated fats, which can lead to insulin resistance, a condition where the body's cells do not respond effectively to insulin. Over time, the pancreas cannot produce enough insulin to manage blood sugar levels, resulting in type 2 diabetes. Additionally, excessive caloric intake without sufficient physical activity exacerbates the risk by promoting weight gain and fat accumulation, particularly around the abdomen, further contributing to insulin resistance.\n",
        "\\nExample 3:\n",
        "Query: What is the importance of hydration for physical performance?\n",
        "Answer: Hydration is crucial for physical performance because water plays key roles in maintaining blood volume, regulating body temperature, and ensuring the transport of nutrients and oxygen to cells. Adequate hydration is essential for optimal muscle function, endurance, and recovery. Dehydration can lead to decreased performance, fatigue, and increased risk of heat-related illnesses, such as heat stroke. Drinking sufficient water before, during, and after exercise helps ensure peak physical performance and recovery.\n",
        "\\nNow use the following context items to answer the user query:\n",
        "{context}\n",
        "\\nRelevant passages: <extract relevant passages from the context here>\n",
        "User query: {query}\n",
        "Answer:\"\"\"\n",
        "\n",
        "    # Update base prompt with context items and query\n",
        "    base_prompt = base_prompt.format(context=context, query=query)\n",
        "\n",
        "    # Create prompt template for instruction-tuned model\n",
        "    dialogue_template = [\n",
        "        {\"role\": \"user\",\n",
        "        \"content\": base_prompt}\n",
        "    ]\n",
        "\n",
        "    # Apply the chat template\n",
        "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
        "                                          tokenize=False,\n",
        "                                          add_generation_prompt=True)\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOdhM5P3D1bX"
      },
      "source": [
        "Looking good! Let's try our function out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_678p3yD1bX"
      },
      "outputs": [],
      "source": [
        "query = random.choice(query_list)\n",
        "print(f\"Query: {query}\")\n",
        "\n",
        "# Get relevant resources\n",
        "scores, indices = retrieve_relevant_resources(query=query,\n",
        "                                              embeddings=embeddings)\n",
        "\n",
        "# Create a list of context items\n",
        "context_items = [pages_and_chunks[i] for i in indices]\n",
        "\n",
        "# Format prompt with context items\n",
        "prompt = prompt_formatter(query=query,\n",
        "                          context_items=context_items)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFzOF-NaD1bY"
      },
      "source": [
        "What a good looking prompt!\n",
        "\n",
        "We can tokenize this and pass it straight to our LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-ovwxUbD1bY"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Generate an output of tokens\n",
        "outputs = llm_model.generate(**input_ids,\n",
        "                             temperature=0.7, # lower temperature = more deterministic outputs, higher temperature = more creative outputs\n",
        "                             do_sample=True, # whether or not to use sampling, see https://huyenchip.com/2024/01/16/sampling.html for more\n",
        "                             max_new_tokens=256) # how many new tokens to generate from prompt\n",
        "\n",
        "# Turn the output tokens into text\n",
        "output_text = tokenizer.decode(outputs[0])\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"RAG answer:\\n{output_text.replace(prompt, '')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_-HnfVrD1bY"
      },
      "source": [
        "Yesssssss!!!\n",
        "\n",
        "Our RAG pipeline is complete!\n",
        "\n",
        "We just Retrieved, Augmented and Generated!\n",
        "\n",
        "And all on our own local GPU!\n",
        "\n",
        "How about we functionize the generation step to make it easier to use?\n",
        "\n",
        "We can put a little formatting on the text being returned to make it look nice too.\n",
        "\n",
        "And we'll make an option to return the context items if needed as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOIBsbCSD1bZ"
      },
      "outputs": [],
      "source": [
        "def ask(query,\n",
        "        temperature=0.7,\n",
        "        max_new_tokens=512,\n",
        "        format_answer_text=True,\n",
        "        return_answer_only=True):\n",
        "    \"\"\"\n",
        "    Takes a query, finds relevant resources/context and generates an answer to the query based on the relevant resources.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get just the scores and indices of top related results\n",
        "    scores, indices = retrieve_relevant_resources(query=query,\n",
        "                                                  embeddings=embeddings)\n",
        "\n",
        "    # Create a list of context items\n",
        "    context_items = [pages_and_chunks[i] for i in indices]\n",
        "\n",
        "    # Add score to context item\n",
        "    for i, item in enumerate(context_items):\n",
        "        item[\"score\"] = scores[i].cpu() # return score back to CPU\n",
        "\n",
        "    # Format the prompt with context items\n",
        "    prompt = prompt_formatter(query=query,\n",
        "                              context_items=context_items)\n",
        "\n",
        "    # Tokenize the prompt\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate an output of tokens\n",
        "    outputs = llm_model.generate(**input_ids,\n",
        "                                 temperature=temperature,\n",
        "                                 do_sample=True,\n",
        "                                 max_new_tokens=max_new_tokens)\n",
        "\n",
        "    # Turn the output tokens into text\n",
        "    output_text = tokenizer.decode(outputs[0])\n",
        "\n",
        "    if format_answer_text:\n",
        "        # Replace special tokens and unnecessary help message\n",
        "        output_text = output_text.replace(prompt, \"\").replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"Sure, here is the answer to the user query:\\n\\n\", \"\")\n",
        "\n",
        "    # Only return the answer without the context items\n",
        "    if return_answer_only:\n",
        "        return output_text\n",
        "\n",
        "    return output_text, context_items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_4wUr0xD1bZ"
      },
      "source": [
        "What a good looking function!\n",
        "\n",
        "The workflow could probably be a little refined but this should work!\n",
        "\n",
        "Let's try it out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdrSu77iD1bb"
      },
      "outputs": [],
      "source": [
        "query = random.choice(query_list)\n",
        "print(f\"Query: {query}\")\n",
        "\n",
        "# Answer query with context and return context\n",
        "answer, context_items = ask(query=query,\n",
        "                            temperature=0.7,\n",
        "                            max_new_tokens=512,\n",
        "                            return_answer_only=False)\n",
        "\n",
        "print(f\"Answer:\\n\")\n",
        "print_wrapped(answer)\n",
        "print(f\"Context items:\")\n",
        "context_items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu9JBsfwD1bc"
      },
      "source": [
        "Local RAG workflow complete!\n",
        "\n",
        "We've now officially got a way to Retrieve, Augment and Generate answers based on a source.\n",
        "\n",
        "For now we can verify our answers manually by reading them and reading through the textbook.\n",
        "\n",
        "But if you want to put this into a production system, it'd be a good idea to have some kind of evaluation on how well our pipeline works.\n",
        "\n",
        "For example, you could use another LLM to rate the answers returned by our LLM and then use those ratings as a proxy evaluation.\n",
        "\n",
        "However, I'll leave this and a few more interesting ideas as extensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvFeNL9SD1bc"
      },
      "source": [
        "## Extensions\n",
        "\n",
        "* May want to improve text extraction with something like Marker - https://github.com/VikParuchuri/marker\n",
        "* Guide to more advanced PDF extraction - https://towardsdatascience.com/extracting-text-from-pdf-files-with-python-a-comprehensive-guide-9fc4003d517\n",
        "* See the following prompt engineering resources for more prompting techniques - promptinguide.ai, Brex's Prompt Engineering Guide\n",
        "* What happens when a query comes through that there isn't any context in the textbook on?\n",
        "* Try another embedding model (e.g. Mixed Bread AI large, `mixedbread-ai/mxbai-embed-large-v1`, see: https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1)\n",
        "* Try another LLM... (e.g. Mistral-Instruct)\n",
        "* Try different prompts (e.g. see prompting techniques online)\n",
        "* Our example only focuses on text from a PDF, however, we could extend it to include figures and images\n",
        "* Evaluate the answers -> could use another LLM to rate our answers (e.g. use GPT-4 to make)\n",
        "* Vector database/index for larger setup (e.g. 100,000+ chunks)\n",
        "* Libraries/frameworks such as LangChain / LlamaIndex can help do many of the steps for you - so it's worth looking into those next, wanted to recreate a workflow with lower-level tools to show the principles\n",
        "* Optimizations for speed\n",
        "    * See Hugging Face docs for recommended speed ups on GPU - https://huggingface.co/docs/transformers/perf_infer_gpu_one\n",
        "    * Optimum NVIDIA - https://huggingface.co/blog/optimum-nvidia, GitHub: https://github.com/huggingface/optimum-nvidia\n",
        "    * See NVIDIA TensorRT-LLM - https://github.com/NVIDIA/TensorRT-LLM\n",
        "    * See GPT-Fast for PyTorch-based optimizations - https://github.com/pytorch-labs/gpt-fast\n",
        "    * Flash attention 2 (requires Ampere GPUs or newer) - https://github.com/Dao-AILab/flash-attention\n",
        "* Stream text output so it looks prettier (e.g. each token appears as it gets output from the model)\n",
        "* Turn the workflow into an app, see Gradio type chatbots for this - https://www.gradio.app/guides/creating-a-chatbot-fast, see local example: https://www.gradio.app/guides/creating-a-chatbot-fast#example-using-a-local-open-source-llm-with-hugging-face"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "90a7c868690049edb669f86b12e30617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b583b39637114cb6a3567d5af882f58a",
              "IPY_MODEL_9eea09cda3f6443d9bbc31ecc4e3987e",
              "IPY_MODEL_047a93da95ca4850a7dd069d7040c548"
            ],
            "layout": "IPY_MODEL_b26186172935433495261e8d8ace0c2c"
          }
        },
        "b583b39637114cb6a3567d5af882f58a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0742ef10f2504b19983c164c20c5f3da",
            "placeholder": "​",
            "style": "IPY_MODEL_65a83ebe560c47118cb08cf4d22d1fa1",
            "value": ""
          }
        },
        "9eea09cda3f6443d9bbc31ecc4e3987e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcf227ef689b4810ad17b7f4d71c875c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cae1da4aea164f4bb71710574d679882",
            "value": 1
          }
        },
        "047a93da95ca4850a7dd069d7040c548": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fedf0fd1ce743beb579c334b78bb783",
            "placeholder": "​",
            "style": "IPY_MODEL_aa6d46699c4f40d2bff242a5c484221b",
            "value": " 113/? [00:00&lt;00:00, 167.91it/s]"
          }
        },
        "b26186172935433495261e8d8ace0c2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0742ef10f2504b19983c164c20c5f3da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65a83ebe560c47118cb08cf4d22d1fa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bcf227ef689b4810ad17b7f4d71c875c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "cae1da4aea164f4bb71710574d679882": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0fedf0fd1ce743beb579c334b78bb783": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa6d46699c4f40d2bff242a5c484221b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1850bdb0e5d84589a4da19aa6cbeefc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_10e1e5cf93064bccaf33ed393aa43a4b",
              "IPY_MODEL_8905d8a08c7a4f319ff0774827899613",
              "IPY_MODEL_18117bd85de44aa0b1cd14e764aedf2c"
            ],
            "layout": "IPY_MODEL_8297bf817f96432c87c9ae30e2f5de41"
          }
        },
        "10e1e5cf93064bccaf33ed393aa43a4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abb3a4a0178e4a829d28589df4d83ddc",
            "placeholder": "​",
            "style": "IPY_MODEL_0228176828294d36b50dceade40651f3",
            "value": "100%"
          }
        },
        "8905d8a08c7a4f319ff0774827899613": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0869e408044433bbf923ccec6421e32",
            "max": 113,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cff986b6329143a5a2dd1b8ff342b09f",
            "value": 113
          }
        },
        "18117bd85de44aa0b1cd14e764aedf2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_748c8be1f753483d980216e421b1645a",
            "placeholder": "​",
            "style": "IPY_MODEL_bf9e83f2fbc44b9db4485f61bc7cc9b9",
            "value": " 113/113 [00:00&lt;00:00, 124.76it/s]"
          }
        },
        "8297bf817f96432c87c9ae30e2f5de41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abb3a4a0178e4a829d28589df4d83ddc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0228176828294d36b50dceade40651f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0869e408044433bbf923ccec6421e32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cff986b6329143a5a2dd1b8ff342b09f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "748c8be1f753483d980216e421b1645a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf9e83f2fbc44b9db4485f61bc7cc9b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d0f1ff0ebe343058227c4ee3976e7b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b5cbab46ddd843a7b87612a8e9c6fff6",
              "IPY_MODEL_05a26620d95741528e71bc2fabc7331f",
              "IPY_MODEL_fb31e7c5831d4b1da8c93458fa3d689b"
            ],
            "layout": "IPY_MODEL_9594fe8e2ae848078a6f4bdc0e208ad5"
          }
        },
        "b5cbab46ddd843a7b87612a8e9c6fff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d76f769c57640d9957a69a5afe420bb",
            "placeholder": "​",
            "style": "IPY_MODEL_5f5f202b60da4e80ab56c65b144997c8",
            "value": "  0%"
          }
        },
        "05a26620d95741528e71bc2fabc7331f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8f0fa81730e4d5297bf2024922214c7",
            "max": 113,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ecc3073000ae4683ace41ddec3ae7d1a",
            "value": 0
          }
        },
        "fb31e7c5831d4b1da8c93458fa3d689b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45f0e9eecfc442708a292aaa474f4742",
            "placeholder": "​",
            "style": "IPY_MODEL_ddc4570c785f4d24b35cfd6f7b4667a5",
            "value": " 0/113 [00:00&lt;?, ?it/s]"
          }
        },
        "9594fe8e2ae848078a6f4bdc0e208ad5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d76f769c57640d9957a69a5afe420bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f5f202b60da4e80ab56c65b144997c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8f0fa81730e4d5297bf2024922214c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecc3073000ae4683ace41ddec3ae7d1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "45f0e9eecfc442708a292aaa474f4742": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddc4570c785f4d24b35cfd6f7b4667a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}